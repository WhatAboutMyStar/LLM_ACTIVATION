{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "205dbf43-0e51-4f7e-ac08-c312d42f4add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmfact import LayerOutputExtractor, FBNFeatureExtractor, GroupFBNFeatureExtractor, FBNExtractor, LLMFC\n",
    "from llmfact.mask import MaskedGPT2ForSequenceClassification, MaskedGPT2AmplifiedForSequenceClassification, MaskedGPT2LMModel\n",
    "from transformers import GPT2Model, GPT2Config, GPT2LMHeadModel, GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, AutoModelForCausalLM, AutoModelForQuestionAnswering\n",
    "from datasets import load_dataset, get_dataset_config_names, get_dataset_split_names\n",
    "from torch.utils.data import DataLoader\n",
    "from rouge_score import rouge_scorer\n",
    "from evaluate import load\n",
    "\n",
    "from llmfact.utils import IoU, correlation_activation, thresholding, write_layer_txt, evaluate_iou\n",
    "from llmfact.stat import  StatICA, StatDictionaryLearning\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import FastICA\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fff87c7-b9de-46fb-82eb-96166b570a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data3/liuyiheng/anaconda3/envs/torch/lib/python3.12/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81762ac02d4c4120854cc59b7a6ab0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"THUDM/chatglm3-6b-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8313c776-93ff-4f3c-b103-5a138fa4276b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97b373f7-5f89-4420-9de8-606ea54520bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: J. K. Rowling\n",
      "CPU times: user 17.2 s, sys: 1.18 s, total: 18.4 s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_ids = tokenizer.encode(\"Harry Potter is a series of seven fantasy novels written by British author, [HL]J. K. Rowling[HL]. Who wrote Harry Potter? \", return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids.to(model.device), \n",
    "                         max_new_tokens=50)\n",
    "print(\"Generated:\", tokenizer.decode(outputs[0][34:], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f3e7c7-ab0a-4d79-9d62-56e82280e618",
   "metadata": {},
   "source": [
    "## SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89ac13c2-91be-4d58-8980-509d1f507e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"rajpurkar/squad\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fade6f9-3420-40d8-b440-bb6cd56bacbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dict, tokenizer, max_length=800):\n",
    "        self.context = data_dict['context']\n",
    "        self.question = data_dict['question']\n",
    "        self.answers = data_dict['answers']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.context)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context = self.context[idx]\n",
    "        question = self.question[idx]\n",
    "        answers = self.answers[idx]['text']\n",
    "        if len(answers) < 4:\n",
    "            answers += [\"0\"] * (4 - len(answers))\n",
    "        else:\n",
    "            answers = answers[:4]\n",
    "        prompt = f\"Please answer the question according to the following context. Just answer in brief only one sentence is enough. \\n Context: {context} \\n Question:{question} \\n Answer:\"\n",
    "        encoding = self.tokenizer(prompt, return_tensors=\"pt\", max_length=self.max_length, truncation=True, padding=\"max_length\")\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'answers': answers\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db3704a8-d2b1-4d7c-ba70-0f7cd492119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exact_match(prediction, ground_truths):\n",
    "    prediction = prediction.lower().strip()\n",
    "    for gt in ground_truths:\n",
    "        gt = gt.lower().strip()\n",
    "        if prediction == gt:\n",
    "            return 1\n",
    "    return 0\n",
    "    \n",
    "import collections\n",
    "def compute_f1(prediction, ground_truths):\n",
    "    def _f1_score(pred_tokens, gold_tokens):\n",
    "        common = collections.Counter(pred_tokens) & collections.Counter(gold_tokens)\n",
    "        num_same = sum(common.values())\n",
    "        if num_same == 0:\n",
    "            return 0\n",
    "        precision = 1.0 * num_same / len(pred_tokens)\n",
    "        recall = 1.0 * num_same / len(gold_tokens)\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "        return f1\n",
    "    \n",
    "    prediction = prediction.lower().strip()\n",
    "    pred_tokens = prediction.split()\n",
    "    \n",
    "    max_f1 = 0\n",
    "    for gt in ground_truths:\n",
    "        gt = gt.lower().strip()\n",
    "        gold_tokens = gt.split()\n",
    "        f1 = _f1_score(pred_tokens, gold_tokens)\n",
    "        max_f1 = max(max_f1, f1)\n",
    "    \n",
    "    return max_f1\n",
    "def transpose_answers(batch_answers):\n",
    "    transposed_answers = list(map(list, zip(*batch_answers)))\n",
    "    return transposed_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d0d4e91-a75b-43ab-97bb-23e5c34827c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, test_loader, batch_size=12):\n",
    "    exact_matches = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    test_dataset = SquadDataset(test_loader, tokenizer, max_length=600)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = model.device\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            batch['answers'] = transpose_answers(batch['answers'])\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            generated_sequences = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=30,\n",
    "            )\n",
    "            \n",
    "            generated_answers = []\n",
    "            for i in range(len(generated_sequences)):\n",
    "                input_length = input_ids.shape[1]\n",
    "                generated_answer = tokenizer.decode(\n",
    "                    generated_sequences[i][input_length:], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                generated_answers.append(generated_answer)\n",
    "                \n",
    "            for i in range(len(generated_answers)):\n",
    "                generated_answer = generated_answers[i]\n",
    "                reference_answers = batch['answers'][i]\n",
    "\n",
    "                em_score = compute_exact_match(generated_answer, reference_answers)\n",
    "                f1_score = compute_f1(generated_answer, reference_answers)\n",
    "                \n",
    "                exact_matches.append(em_score)\n",
    "                f1_scores.append(f1_score)\n",
    "    \n",
    "    avg_em = np.mean(exact_matches)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    \n",
    "    print(f\"Average Exact Match: {avg_em:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "\n",
    "    return avg_em, avg_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd952807-2f7b-44d5-94f4-ecfe429f8737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c46d89142db47d4bd69e0b2c413a2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/331 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Exact Match: 0.7876\n",
      "Average F1 Score: 0.9021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.7876064333017976), np.float64(0.9020856125791711))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, tokenizer, dataset['validation'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd1a6b4d-61ba-40a7-8d97-7a85cc8457e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGLMForConditionalGeneration(\n",
       "  (transformer): ChatGLMModel(\n",
       "    (embedding): Embedding(\n",
       "      (word_embeddings): Embedding(65024, 4096)\n",
       "    )\n",
       "    (rotary_pos_emb): RotaryEmbedding()\n",
       "    (encoder): GLMTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-27): 28 x GLMBlock(\n",
       "          (input_layernorm): RMSNorm()\n",
       "          (self_attention): SelfAttention(\n",
       "            (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)\n",
       "            (core_attention): CoreAttention(\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (dense): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (post_attention_layernorm): RMSNorm()\n",
       "          (mlp): MLP(\n",
       "            (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)\n",
       "            (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layernorm): RMSNorm()\n",
       "    )\n",
       "    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dba7de6-7098-4ddf-89c3-3d9f4aeaa0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer.encoder.layers.0.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.1.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.2.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.3.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.4.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.5.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.6.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.7.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.8.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.9.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.10.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.11.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.12.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.13.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.14.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.15.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.16.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.17.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.18.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.19.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.20.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.21.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.22.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.23.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.24.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.25.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.26.mlp.dense_4h_to_h',\n",
       " 'transformer.encoder.layers.27.mlp.dense_4h_to_h']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include_layers = []\n",
    "for name, _ in model.named_modules():\n",
    "    if \"mlp.dense_4h_to_h\" in name:\n",
    "        include_layers.append(name)\n",
    "include_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1414979-bd1a-4679-8434-d19d41df81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedModel(MaskedGPT2LMModel):\n",
    "    def __init__(self, model, include_layers=[]):\n",
    "        super().__init__(model, include_layers)\n",
    "\n",
    "    def forward(self, \n",
    "                input_ids,\n",
    "                attention_mask,\n",
    "                max_new_tokens=50):\n",
    "        with torch.no_grad():\n",
    "            generated_output = self.model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=max_new_tokens\n",
    "            )\n",
    "        return generated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "623f30e4-9e20-4bf4-9f51-991f5e38bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate2(model, tokenizer, include_layers, mask, test_loader, batch_size=32):\n",
    "    exact_matches = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    test_dataset = SquadDataset(test_loader, tokenizer, max_length=600)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = model.device\n",
    "    model.eval()\n",
    "\n",
    "    masked_model = MaskedModel(model, include_layers)\n",
    "    mask = torch.tensor(mask.reshape(28, 4096), dtype=torch.bool).to(device)\n",
    "    masked_model.register_hooks(mask)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            batch['answers'] = transpose_answers(batch['answers'])\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            generated_sequences = masked_model.forward(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=30\n",
    "            )\n",
    "            \n",
    "            generated_answers = []\n",
    "            for i in range(len(generated_sequences)):\n",
    "                input_length = input_ids.shape[1]\n",
    "                generated_answer = tokenizer.decode(\n",
    "                    generated_sequences[i][input_length:], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                generated_answers.append(generated_answer)\n",
    "                \n",
    "            for i in range(len(generated_answers)):\n",
    "                generated_answer = generated_answers[i]\n",
    "                reference_answers = batch['answers'][i]\n",
    "\n",
    "                em_score = compute_exact_match(generated_answer, reference_answers)\n",
    "                f1_score = compute_f1(generated_answer, reference_answers)\n",
    "                \n",
    "                exact_matches.append(em_score)\n",
    "                f1_scores.append(f1_score)\n",
    "    masked_model.remove_hooks()\n",
    "    avg_em = np.mean(exact_matches)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    \n",
    "    print(f\"Average Exact Match: {avg_em:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "\n",
    "    return avg_em, avg_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d85ca02c-1e46-4a05-b569-8f39071ae2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hooks(module):\n",
    "    if hasattr(module, \"_forward_hooks\"):\n",
    "        module._forward_hooks.clear()\n",
    "    if hasattr(module, \"_backward_hooks\"):\n",
    "        module._backward_hooks.clear()\n",
    "    for child in module.children():\n",
    "        remove_hooks(child)\n",
    "\n",
    "remove_hooks(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95d03f92-9672-4e43-a12e-bdb345536e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 72,  93,  84,  78,  86,  92,  76,  85,  78,  76,  84,  87,  96, 103,\n",
      "         78,  68,  68,  80,  97, 102,  86,  84,  79,  88,  74,  80,  81,  83])\n",
      "tensor(2338)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2166319/3525715426.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = torch.tensor(mask.reshape(28, 4096), dtype=torch.bool).to(device)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ea1177db304050898d343c93159fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/331 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Exact Match: 0.7901\n",
      "Average F1 Score: 0.9036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.7900662251655629), np.float64(0.9036269946917632))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_matrix = torch.rand(28, 4096) < 0.02\n",
    "print(mask_matrix.reshape(28, 4096).sum(axis=1))\n",
    "print(mask_matrix.sum())\n",
    "evaluate2(model, tokenizer, include_layers, mask_matrix, dataset['validation'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc5b2313-1f78-4495-b676-edc892b9d563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 114688)\n",
      "(1, 114688)\n",
      "[  4   1   4   1   1   1   1   0   0   0   1   0   1   1   1   2   2   4\n",
      "  22  31  94 105 206 287 425 495 607 552]\n",
      "[2849]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413768b69a7e4949bc86b17b26026b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/294 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Exact Match: 0.0000\n",
      "Average F1 Score: 0.0358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a242ec579c4e69bb8a6897d6afa231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/294 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Exact Match: 0.0000\n",
      "Average F1 Score: 0.0001\n",
      "CPU times: user 22h 6min 4s, sys: 4min 32s, total: 22h 10min 37s\n",
      "Wall time: 13h 3min 49s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.0), np.float64(6.141418430419078e-05))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "inputs_list = []\n",
    "for i in range(10):\n",
    "    inputs = tokenizer(f\"Please answer the question according to the following context. Just answer in brief only one sentence is enough. \\n Context: {dataset['train']['context'][i]} \\n Question:{dataset['train']['question'][i]} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "# inputs_list = [tokenizer(inputs, return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['article'][:100]]\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=10, alpha=3.6, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum(axis=1))\n",
    "evaluate2(model, tokenizer, include_layers, any_mask, dataset['validation'], batch_size=36)\n",
    "evaluate2(model, tokenizer, include_layers, ~any_mask, dataset['validation'], batch_size=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5079b83-9d6b-4180-b759-066d61029808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 114688)\n",
      "(1, 114688)\n",
      "[   7    1    6    1    1    1    1    1    1    1    1    1    2    1\n",
      "    1    3    4   16   34  102  443  622 1161 1713 2452 3010 3185 3378]\n",
      "[16150]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed96eb41c0943dc975f37a929643f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/279 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Exact Match: 0.7258\n",
      "Average F1 Score: 0.8655\n",
      "CPU times: user 14h 50min 54s, sys: 2min 13s, total: 14h 53min 7s\n",
      "Wall time: 6h 29min 58s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.7258278145695364), np.float64(0.8655080424049906))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "inputs_list = []\n",
    "for i in range(10):\n",
    "    inputs = tokenizer(f\"Please answer the question according to the following context. Just answer in brief only one sentence is enough. \\n Context: {dataset['train']['context'][i]} \\n Question:{dataset['train']['question'][i]} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "# inputs_list = [tokenizer(inputs, return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['article'][:100]]\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=64, alpha=3.6, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum(axis=1))\n",
    "evaluate2(model, tokenizer, include_layers, ~any_mask, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4a6e159-d959-48f9-b033-3edca0de39a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 114688)\n",
      "(1, 114688)\n",
      "[   8    2    4    0    1    1    1    1    1    1    1    1    5    2\n",
      "    4    2    7   10   50  180  648 1059 1890 2665 3507 3871 3962 4019]\n",
      "[21903]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285af5a263a3418d903d1742e8b8bc13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/279 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Exact Match: 0.7893\n",
      "Average F1 Score: 0.9016\n",
      "CPU times: user 16h 21min 42s, sys: 2min 20s, total: 16h 24min 2s\n",
      "Wall time: 6h 34min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.7893093661305581), np.float64(0.9016385431285916))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "inputs_list = []\n",
    "for i in range(10):\n",
    "    inputs = tokenizer(f\"Please answer the question according to the following context. Just answer in brief only one sentence is enough. \\n Context: {dataset['train']['context'][i]} \\n Question:{dataset['train']['question'][i]} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "# inputs_list = [tokenizer(inputs, return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['article'][:100]]\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=128, alpha=3.6, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum(axis=1))\n",
    "evaluate2(model, tokenizer, include_layers, ~any_mask, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5b3f5be-709f-4e68-822e-f35161c986f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 114688)\n",
      "(1, 114688)\n",
      "[  10    4    5    2    2    2    1    1    1    1    1    1    4    2\n",
      "    3    5   13   37  134  348 1087 1939 2965 3753 4049 4088 4093 4095]\n",
      "[26646]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef86a8463c444f7928fefa5e9f8073f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/279 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Exact Match: 0.7872\n",
      "Average F1 Score: 0.9023\n",
      "CPU times: user 16h 30min 23s, sys: 2min 19s, total: 16h 32min 43s\n",
      "Wall time: 6h 35min 17s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.7872280037842951), np.float64(0.9022577696457327))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "inputs_list = []\n",
    "for i in range(10):\n",
    "    inputs = tokenizer(f\"Please answer the question according to the following context. Just answer in brief only one sentence is enough. \\n Context: {dataset['train']['context'][i]} \\n Question:{dataset['train']['question'][i]} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "# inputs_list = [tokenizer(inputs, return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['article'][:100]]\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=256, alpha=3.6, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum(axis=1))\n",
    "evaluate2(model, tokenizer, include_layers, ~any_mask, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31d71e63-a83f-4864-9c63-44004ea300bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 114688)\n",
      "(1, 114688)\n",
      "[  11    4    6    2    2    2    2    2    1    1    1    1    5    3\n",
      "    5   17   61  189  325  781 1874 2806 3698 4045 4094 4096 4096 4096]\n",
      "[30226]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68eec95f05b411683a5cf160c034014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/279 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Exact Match: 0.7876\n",
      "Average F1 Score: 0.9021\n",
      "CPU times: user 20h 45min 14s, sys: 3min 5s, total: 20h 48min 19s\n",
      "Wall time: 6h 52min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.7876064333017976), np.float64(0.902072998261921))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "inputs_list = []\n",
    "for i in range(10):\n",
    "    inputs = tokenizer(f\"Please answer the question according to the following context. Just answer in brief only one sentence is enough. \\n Context: {dataset['train']['context'][i]} \\n Question:{dataset['train']['question'][i]} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "# inputs_list = [tokenizer(inputs, return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['article'][:100]]\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=512, alpha=3.6, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum(axis=1))\n",
    "evaluate2(model, tokenizer, include_layers, ~any_mask, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dff7ba-b05d-46d2-bac5-42f7b9dcaab3",
   "metadata": {},
   "source": [
    "## SST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd11fd96-bc0c-418d-b774-ebe53525f33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"glue\", 'sst2')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83fd658a-3f9a-453f-a6ca-0857cd506a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SST2Dataset(Dataset):\n",
    "    def __init__(self, data_dict, tokenizer, max_length=512):\n",
    "        self.sentence = data_dict['sentence']\n",
    "        self.label = data_dict['label']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # self.dictionary = {\n",
    "        #     0: \"negative\",\n",
    "        #     1: \"positive\"\n",
    "        # }\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentence[idx]\n",
    "        label = self.label[idx]\n",
    "\n",
    "        prompt = f\"Given the sentence '{sentence}', it expresses a sentiment of positive or negative. You only need to answer positive or negative. Answer:\"\n",
    "        encoding = self.tokenizer(prompt, return_tensors=\"pt\", max_length=self.max_length, truncation=True, padding=\"max_length\")\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef6b95ce-5719-4335-b471-5c192500539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(generated_answer):\n",
    "    if \"positive\" in generated_answer.lower():\n",
    "        return 1\n",
    "    elif \"negative\" in generated_answer.lower():\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcd22a64-6ede-4928-b857-c54fc9e37c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import trange\n",
    "def evaluate_sst2(model, include_layers, tokenizer, test_loader, batch_size=32):\n",
    "    test_dataset = SST2Dataset(test_loader, tokenizer, max_length=512)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = model.device\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = test_loader['label']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            generated_sequences = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=50\n",
    "            )\n",
    "            \n",
    "            generated_answers = []\n",
    "            for i in range(len(generated_sequences)):\n",
    "                input_length = input_ids.shape[1]\n",
    "                generated_answer = tokenizer.decode(\n",
    "                    generated_sequences[i][input_length:], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                generated_answers.append(generated_answer)\n",
    "            for i in range(len(generated_answers)):\n",
    "                generated_answer = generated_answers[i]\n",
    "                generated_answer = predict_sentiment(generated_answer)\n",
    "                    \n",
    "                reference_answers = batch['label'][i]\n",
    "                if generated_answer == -1:\n",
    "                    if reference_answers == 0:\n",
    "                        generated_answer = 1\n",
    "                    else:\n",
    "                        generated_answer = 0\n",
    "                all_predictions.append(generated_answer)\n",
    "    \n",
    "    acc = accuracy_score(all_predictions, all_labels)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa983753-029e-4048-89ea-8ed29d036805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685980ef584a4bd696c637d7c894459a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9392201834862385"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_sst2(model, include_layers, tokenizer, dataset['validation'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02f9bd49-1bd2-433f-a196-3b47b2c9b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_masked_sst2(model, include_layers, mask, tokenizer, test_loader, batch_size=36):\n",
    "    test_dataset = SST2Dataset(test_loader, tokenizer, max_length=512)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    mask = torch.tensor(mask.reshape(28, 4096), dtype=bool)\n",
    "    device = model.device\n",
    "    model.eval()\n",
    "    masked_model = MaskedModel(model, include_layers)\n",
    "    masked_model.register_hooks(mask)\n",
    "    all_predictions = []\n",
    "    all_labels = test_loader['label']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            generated_sequences = masked_model.forward(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=30\n",
    "            )\n",
    "            \n",
    "            generated_answers = []\n",
    "            for i in range(len(generated_sequences)):\n",
    "                input_length = input_ids.shape[1]\n",
    "                generated_answer = tokenizer.decode(\n",
    "                    generated_sequences[i][input_length:], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                generated_answers.append(generated_answer)\n",
    "            for i in range(len(generated_answers)):\n",
    "                generated_answer = generated_answers[i]\n",
    "                generated_answer = predict_sentiment(generated_answer)\n",
    "                    \n",
    "                reference_answers = batch['label'][i]\n",
    "                if generated_answer == -1:\n",
    "                    if reference_answers == 0:\n",
    "                        generated_answer = 1\n",
    "                    else:\n",
    "                        generated_answer = 0\n",
    "                all_predictions.append(generated_answer)\n",
    "    masked_model.remove_hooks()\n",
    "    acc = accuracy_score(all_predictions, all_labels)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "525a73c7-dec8-4ac1-9d7f-2c7bed132c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0a03464-ab5c-4a68-bc02-129a65d8ea53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 114688)\n",
      "(1, 114688)\n",
      "[  6   2   4   1   0   1   0   1   1   0   0   1   3   2   3   4   5  29\n",
      "   7  23  10  12  15  11  16  69 350 659]\n",
      "1235\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3da12b5e3e74e48a156ddaef7617f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a92c6eece9c41e6aa3869efa2785c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1422\n",
      "CPU times: user 10h 2min 43s, sys: 1min 32s, total: 10h 4min 15s\n",
      "Wall time: 1h 28min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14220183486238533"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "inputs_list = [tokenizer(inputs, return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['validation']['sentence'][:100]]\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=10, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_sst2(model, include_layers, ~any_mask, tokenizer, dataset['validation'], batch_size=38)\n",
    "evaluate_masked_sst2(model, include_layers, any_mask, tokenizer, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dd698d9-1a76-4b13-8548-24160bc5078f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 114688)\n",
      "(1, 114688)\n",
      "[   9    4    5    2    1    1    1    1    1    1    2    1    5    5\n",
      "    5   10    9   77   53  261  521  342  340  294  423 1518 2302 3196]\n",
      "9390\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79a2aadeed046afb9af7b4cba6d1f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8624\n",
      "CPU times: user 9h 32min 33s, sys: 1min 24s, total: 9h 33min 57s\n",
      "Wall time: 58min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8623853211009175"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "inputs_list = [tokenizer(inputs, return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['validation']['sentence'][:100]]\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=64, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_sst2(model, include_layers, ~any_mask, tokenizer, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82a90c5a-bda1-4624-8d06-1aba74ec06ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 114688)\n",
      "(1, 114688)\n",
      "[   9    3    5    1    1    1    1    1    1    1    1    2    5    5\n",
      "   20    9   11  131  178  487  977  675  749  779  911 2699 3359 3832]\n",
      "14854\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9aa65717c2b4ebaa2c7147db2e08aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9300\n",
      "CPU times: user 10h 12min 15s, sys: 1min 24s, total: 10h 13min 39s\n",
      "Wall time: 1h 1min 32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.930045871559633"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "inputs_list = [tokenizer(inputs, return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['validation']['sentence'][:100]]\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=128, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_sst2(model, include_layers, ~any_mask, tokenizer, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b039b81-c25e-4eb8-83ee-d800d68b3207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data3/liuyiheng/anaconda3/envs/torch/lib/python3.12/site-packages/sklearn/decomposition/_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 114688)\n",
      "(1, 114688)\n",
      "[  10    3    7    1    1    1    1    1    2    1    2    3    5    4\n",
      "   28    8    9  186  279  745 1253 1013 1087 1317 1578 3306 3806 4045]\n",
      "18702\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a4682772714903a334d13698f42f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9381\n",
      "CPU times: user 11h 57min 41s, sys: 1min 35s, total: 11h 59min 17s\n",
      "Wall time: 1h 8min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9380733944954128"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "inputs_list = [tokenizer(inputs, return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['validation']['sentence'][:100]]\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=256, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_sst2(model, include_layers, ~any_mask, tokenizer, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49f2bb30-de88-4048-94f9-4d8e6ea9caa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data3/liuyiheng/anaconda3/envs/torch/lib/python3.12/site-packages/sklearn/decomposition/_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 114688)\n",
      "(1, 114688)\n",
      "[  12    5    7    1    2    1    2    1    1    1    2    2    6    4\n",
      "   24   16   33  545  583 1508 2080 1996 2036 2384 3063 4024 4079 4095]\n",
      "26513\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe1d8319797843599f0f70f2e32ac9f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9404\n",
      "CPU times: user 19h 12min 16s, sys: 2min 33s, total: 19h 14min 50s\n",
      "Wall time: 1h 36min 41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9403669724770642"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "inputs_list = [tokenizer(inputs, return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['validation']['sentence'][:100]]\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=512, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_sst2(model, include_layers, ~any_mask, tokenizer, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "844ad77c-b67e-4cff-b737-2c97866b195d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([82, 90, 95, 76, 74, 81, 83, 87, 78, 86, 92, 75, 84, 65, 84, 88, 86, 87,\n",
      "        71, 93, 82, 93, 92, 78, 82, 85, 73, 97])\n",
      "tensor(2339)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1523439/4016438927.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = torch.tensor(mask.reshape(28, 4096), dtype=bool)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c719d007380467ba8f05121cb55af42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9380733944954128"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_matrix = torch.rand(28, 4096) < 0.02\n",
    "print(mask_matrix.reshape(28, 4096).sum(axis=1))\n",
    "print(mask_matrix.sum())\n",
    "evaluate_masked_sst2(model, include_layers, mask_matrix, tokenizer, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d91a77-60e8-4619-ac72-64d53502b500",
   "metadata": {},
   "source": [
    "## COLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeb0bfb4-848d-474f-b65c-b37fceebe679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 8551\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1043\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1063\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"nyu-mll/glue\", \"cola\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8e48f60-e13b-4a48-9aae-1315316775e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColaDataset(Dataset):\n",
    "    def __init__(self, data_dict, tokenizer, max_length=70):\n",
    "        self.sentence = data_dict['sentence']\n",
    "        self.label = data_dict['label']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # self.dictionary = {\n",
    "        #     0: \"negative\",\n",
    "        #     1: \"positive\"\n",
    "        # }\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentence[idx]\n",
    "        label = self.label[idx]\n",
    "\n",
    "        prompt = f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. \\n Sentence:{sentence} \\n Please only answer 'acceptable' or 'unacceptable'. Answer:\"\n",
    "        encoding = self.tokenizer(prompt, return_tensors=\"pt\", max_length=self.max_length, truncation=True, padding=\"max_length\")\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "004b02d7-da01-4b7f-8661-4e4a2c5fde8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cola(generated_answer):\n",
    "    if \"acceptable\" in generated_answer.lower():\n",
    "        return 1\n",
    "    elif \"unacceptable\" in generated_answer.lower():\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "426fe013-9fae-4c53-9f29-bd8a01cbe597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cola(model, include_layers, tokenizer, test_loader, batch_size=36):\n",
    "    test_dataset = ColaDataset(test_loader, tokenizer, max_length=70)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = model.device\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = test_loader['label']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            generated_sequences = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=10\n",
    "            )\n",
    "            \n",
    "            generated_answers = []\n",
    "            for i in range(len(generated_sequences)):\n",
    "                input_length = input_ids.shape[1]\n",
    "                generated_answer = tokenizer.decode(\n",
    "                    generated_sequences[i][input_length:], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                generated_answers.append(generated_answer)\n",
    "            for i in range(len(generated_answers)):\n",
    "                generated_answer = generated_answers[i]\n",
    "                generated_answer = predict_cola(generated_answer)\n",
    "                    \n",
    "                reference_answers = batch['label'][i]\n",
    "                if generated_answer == -1:\n",
    "                    if reference_answers == 0:\n",
    "                        generated_answer = 1\n",
    "                    else:\n",
    "                        generated_answer = 0\n",
    "                all_predictions.append(generated_answer)\n",
    "    \n",
    "    acc = accuracy_score(all_predictions, all_labels)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a27f2b0-678a-4961-b21f-2da3301e9dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed20e7d6b0934c66862ccd2ce13d514d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6893576222435283"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_cola(model, include_layers, tokenizer, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de923cea-b288-40d7-abb9-2088f5479167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_masked_cola(model, include_layers, mask, tokenizer, test_loader, batch_size=36):\n",
    "    test_dataset = ColaDataset(test_loader, tokenizer, max_length=80)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    mask = torch.tensor(mask.reshape(28, 4096), dtype=bool)\n",
    "    \n",
    "    masked_model = MaskedModel(model, include_layers)\n",
    "    masked_model.register_hooks(mask)\n",
    "    \n",
    "    device = model.device\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = test_loader['label']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            generated_sequences = masked_model.forward(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=10\n",
    "            )\n",
    "            \n",
    "            \n",
    "            generated_answers = []\n",
    "            for i in range(len(generated_sequences)):\n",
    "                input_length = input_ids.shape[1]\n",
    "                generated_answer = tokenizer.decode(\n",
    "                    generated_sequences[i][input_length:], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                generated_answers.append(generated_answer)\n",
    "            for i in range(len(generated_answers)):\n",
    "                generated_answer = generated_answers[i]\n",
    "                generated_answer = predict_cola(generated_answer)\n",
    "                    \n",
    "                reference_answers = batch['label'][i]\n",
    "                if generated_answer == -1:\n",
    "                    if reference_answers == 0:\n",
    "                        generated_answer = 1\n",
    "                    else:\n",
    "                        generated_answer = 0\n",
    "                all_predictions.append(generated_answer)\n",
    "                \n",
    "    masked_model.remove_hooks()\n",
    "    acc = accuracy_score(all_predictions, all_labels)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ad9c4e8-032f-43d4-a66c-0175dad9a32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 114688)\n",
      "(1, 114688)\n",
      "[   7    1    4    1    1    1    1    1    1    0    1    1    2    1\n",
      "    0    2    1    6    5    6   23   48  109  211  224  450  601 1258]\n",
      "2967\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3934891c79d43f0b1849c2b5ed572bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7dae06847f4fc3a51ab1d2801c917c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0000\n",
      "CPU times: user 2d 15h 14min 50s, sys: 12min 9s, total: 2d 15h 26min 59s\n",
      "Wall time: 4h 20min 8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=10, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_cola(model, include_layers, ~any_mask, tokenizer, dataset['validation'], batch_size=38)\n",
    "evaluate_masked_cola(model, include_layers, any_mask, tokenizer, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eec9b682-b953-4dd2-9f5c-8f34d7bf46c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 114688)\n",
      "(1, 114688)\n",
      "[   8    2    4    1    2    2    2    1    1    1    1    2    5    2\n",
      "    2   20   27   37   51   83  219  307  578  882 1192 2426 2851 3597]\n",
      "12306\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ad1d5c892b45b4bc02abb720998935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6913\n",
      "CPU times: user 2d 14h 24min, sys: 9min 13s, total: 2d 14h 33min 14s\n",
      "Wall time: 4h 10min 52s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6912751677852349"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=64, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_cola(model, include_layers, ~any_mask, tokenizer, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38cb973b-45b1-4144-a934-013d9a64f8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 114688)\n",
      "(1, 114688)\n",
      "[   8    3    4    1    1    2    2    1    1    1    1    2    5    2\n",
      "    4   14   29  106  137  213  447  559  832 1296 1659 3182 3500 3954]\n",
      "15966\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1ad849be4f4b849f115e1cbaa48863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6913\n",
      "CPU times: user 2d 15h 30min 37s, sys: 8min 24s, total: 2d 15h 39min 2s\n",
      "Wall time: 4h 14min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6912751677852349"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=128, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_cola(model, include_layers, ~any_mask, tokenizer, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8cc1cebf-37a3-4067-a9fb-34e56257cd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 114688)\n",
      "(1, 114688)\n",
      "[  10    4    4    3    2    1    1    1    1    1    1    2    5    2\n",
      "    3   24   52  264  311  413  825 1023 1406 1931 2320 3848 3955 4082]\n",
      "20495\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d85f481741483eb9872115fe306d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6913\n",
      "CPU times: user 2d 15h 28min 5s, sys: 8min 45s, total: 2d 15h 36min 50s\n",
      "Wall time: 4h 14min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6912751677852349"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=256, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_cola(model, include_layers, ~any_mask, tokenizer, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "766c8827-93f8-4e3b-a54b-f670af3be365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data3/liuyiheng/anaconda3/envs/torch/lib/python3.12/site-packages/sklearn/decomposition/_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 114688)\n",
      "(1, 114688)\n",
      "[  10    3    7    2    2    1    1    1    1    1    1    2    5    2\n",
      "    7   66  134  736  791  923 1759 2122 2597 3016 3367 4092 4088 4095]\n",
      "27832\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22e428e5cdc430293e4d1e2e07c2b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6913\n",
      "CPU times: user 2d 22h 43min 57s, sys: 9min 8s, total: 2d 22h 53min 5s\n",
      "Wall time: 4h 40min 37s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6912751677852349"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=512, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_cola(model, include_layers, ~any_mask, tokenizer, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05ec60e6-fa06-46aa-8b39-bc2d8a144020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([400, 396, 418, 401, 398, 399, 396, 413, 414, 397, 388, 395, 383, 432,\n",
      "        439, 384, 433, 384, 439, 387, 437, 409, 413, 431, 413, 405, 419, 376])\n",
      "tensor(11399)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1523439/455776000.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = torch.tensor(mask.reshape(28, 4096), dtype=bool)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b81489e52d044249b0610904595691e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5752636625119847"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_matrix = torch.rand(28, 4096) < 0.10\n",
    "print(mask_matrix.reshape(28, 4096).sum(axis=1))\n",
    "print(mask_matrix.sum())\n",
    "evaluate_masked_cola(model, include_layers, mask_matrix, tokenizer, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4721a16f-2737-44f3-9554-fd5ec696f5bf",
   "metadata": {},
   "source": [
    "## MRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3ed1e87-372b-468f-9d23-2ea2270d5abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"nyu-mll/glue\", \"mrpc\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "576021c6-576f-4b5d-bb2d-a624368a5245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRPCDataset(Dataset):\n",
    "    def __init__(self, data_dict, tokenizer, max_length=200):\n",
    "        self.sentence1 = data_dict['sentence1']\n",
    "        self.sentence2 = data_dict['sentence2']\n",
    "        self.label = data_dict['label']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence1 = self.sentence1[idx]\n",
    "        sentence2 = self.sentence2[idx]\n",
    "        label = self.label[idx]\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Task: Compare the two sentences below and determine if they are paraphrases of each other. A paraphrase means that both sentences express the same idea, though they may use different words or structures.\n",
    "        Sentence 1: {sentence1}\n",
    "        Sentence 2: {sentence2}\n",
    "        Please only respond with \"yes\" if they are paraphrases, or \"no\" if they are not.\n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        encoding = self.tokenizer(prompt, return_tensors=\"pt\", max_length=self.max_length, truncation=True, padding=\"max_length\")\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': label\n",
    "        }\n",
    "        \n",
    "def predict_mrpc(generated_answer):\n",
    "    if \"yes\" in generated_answer.lower():\n",
    "        return 1\n",
    "    elif \"no\" in generated_answer.lower():\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "        \n",
    "def evaluate_mrpc(model, include_layers, tokenizer, test_loader, batch_size=38):\n",
    "    test_dataset = MRPCDataset(test_loader, tokenizer, max_length=200)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = model.device\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = test_loader['label']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            generated_sequences = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=10\n",
    "            )\n",
    "            \n",
    "            generated_answers = []\n",
    "            for i in range(len(generated_sequences)):\n",
    "                input_length = input_ids.shape[1]\n",
    "                generated_answer = tokenizer.decode(\n",
    "                    generated_sequences[i][input_length:], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                generated_answers.append(generated_answer)\n",
    "            for i in range(len(generated_answers)):\n",
    "                generated_answer = generated_answers[i]\n",
    "                generated_answer = predict_mrpc(generated_answer)\n",
    "                    \n",
    "                reference_answers = batch['label'][i]\n",
    "                if generated_answer == -1:\n",
    "                    if reference_answers == 0:\n",
    "                        generated_answer = 1\n",
    "                    else:\n",
    "                        generated_answer = 0\n",
    "                all_predictions.append(generated_answer)\n",
    "    \n",
    "    acc = accuracy_score(all_predictions, all_labels)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    return acc\n",
    "\n",
    "def evaluate_masked_mrpc(model, include_layers, mask, tokenizer, test_loader, batch_size=38):\n",
    "    test_dataset = MRPCDataset(test_loader, tokenizer, max_length=200)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    mask = torch.tensor(mask.reshape(28, 4096), dtype=bool)\n",
    "    \n",
    "    masked_model = MaskedModel(model, include_layers)\n",
    "    masked_model.register_hooks(mask)\n",
    "    \n",
    "    device = model.device\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = test_loader['label']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            generated_sequences = masked_model.forward(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=10\n",
    "            )\n",
    "            \n",
    "            generated_answers = []\n",
    "            for i in range(len(generated_sequences)):\n",
    "                input_length = input_ids.shape[1]\n",
    "                generated_answer = tokenizer.decode(\n",
    "                    generated_sequences[i][input_length:], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                generated_answers.append(generated_answer)\n",
    "            for i in range(len(generated_answers)):\n",
    "                generated_answer = generated_answers[i]\n",
    "                generated_answer = predict_mrpc(generated_answer)\n",
    "                    \n",
    "                reference_answers = batch['label'][i]\n",
    "                if generated_answer == -1:\n",
    "                    if reference_answers == 0:\n",
    "                        generated_answer = 1\n",
    "                    else:\n",
    "                        generated_answer = 0\n",
    "                all_predictions.append(generated_answer)\n",
    "                \n",
    "    masked_model.remove_hooks()\n",
    "    acc = accuracy_score(all_predictions, all_labels)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f39e9bb-7fcb-4d12-ac6c-60386ecaa3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b337084ebc347f4871b6e2c63499974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8161764705882353"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_mrpc(model, include_layers, tokenizer, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54e4b872-4b64-4d0e-9f38-674f349a9d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63462af5-9820-429e-90b7-d73e771c20fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 114688)\n",
      "(1, 114688)\n",
      "[   6    1    4    1    1    1    1    1    1    0    1    1    2    1\n",
      "    2    1    2    4   20   21   53   52   94  119  144  367  401 1042]\n",
      "2344\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6ffd6076744331b9e2207cdc830051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7735707779d246faa56514b86203475d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0441\n",
      "CPU times: user 1d 9h 54min 6s, sys: 4min 46s, total: 1d 9h 58min 53s\n",
      "Wall time: 2h 27min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04411764705882353"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "# inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "inputs_list = []\n",
    "for i in range(60):\n",
    "    prompt = f\"\"\"Task: Compare the two sentences below and determine if they are paraphrases of each other. A paraphrase means that both sentences express the same idea, though they may use different words or structures.\n",
    "    Sentence 1: {dataset[\"train\"]['sentence1'][i]}\n",
    "    Sentence 2: {dataset[\"train\"]['sentence2'][i]}\n",
    "    Please only respond with \"yes\" if they are paraphrases, or \"no\" if they are not.\n",
    "    Answer:\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=10, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_mrpc(model, include_layers, ~any_mask, tokenizer, dataset['validation'], batch_size=38)\n",
    "evaluate_masked_mrpc(model, include_layers, any_mask, tokenizer, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "531f9685-58a0-47a1-8aef-870b08b50ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 114688)\n",
      "(1, 114688)\n",
      "[  12    4    5    3    2    1    1    1    1    1    1    2    5    2\n",
      "    3    4    6  132   71  138  279  506  793 1118 1672 2534 2887 3733]\n",
      "13917\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b893d20ea6e14b16b6654d72a3286542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7770\n",
      "CPU times: user 1d 10h 8min 43s, sys: 4min 22s, total: 1d 10h 13min 6s\n",
      "Wall time: 2h 22min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7769607843137255"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "# inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "inputs_list = []\n",
    "for i in range(60):\n",
    "    prompt = f\"\"\"Task: Compare the two sentences below and determine if they are paraphrases of each other. A paraphrase means that both sentences express the same idea, though they may use different words or structures.\n",
    "    Sentence 1: {dataset[\"train\"]['sentence1'][i]}\n",
    "    Sentence 2: {dataset[\"train\"]['sentence2'][i]}\n",
    "    Please only respond with \"yes\" if they are paraphrases, or \"no\" if they are not.\n",
    "    Answer:\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=64, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_mrpc(model, include_layers, ~any_mask, tokenizer, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19bc2436-7af6-4ad6-8f6c-a40601efcc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 114688)\n",
      "(1, 114688)\n",
      "[  10    4    5    2    2    2    2    1    1    1    2    2    5    3\n",
      "    4    7   19  211  166  325  573 1027 1222 1605 2256 3313 3712 4038]\n",
      "18520\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ea811f34744159a3d0aeaf5e18e092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8186\n",
      "CPU times: user 1d 10h 1min 36s, sys: 4min 1s, total: 1d 10h 5min 37s\n",
      "Wall time: 2h 22min 22s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8186274509803921"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "# inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "inputs_list = []\n",
    "for i in range(60):\n",
    "    prompt = f\"\"\"Task: Compare the two sentences below and determine if they are paraphrases of each other. A paraphrase means that both sentences express the same idea, though they may use different words or structures.\n",
    "    Sentence 1: {dataset[\"train\"]['sentence1'][i]}\n",
    "    Sentence 2: {dataset[\"train\"]['sentence2'][i]}\n",
    "    Please only respond with \"yes\" if they are paraphrases, or \"no\" if they are not.\n",
    "    Answer:\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=128, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_mrpc(model, include_layers, ~any_mask, tokenizer, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88fe4b29-96a2-46f7-af2c-7f9e6a52d451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 114688)\n",
      "(1, 114688)\n",
      "[  12    4    5    3    4    1    1    1    1    2    2    2    5    3\n",
      "    3    5   21  307  237  529  991 1745 1784 2440 3194 3914 4061 4092]\n",
      "23369\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410286779270473c9308016e1d2d90ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8162\n",
      "CPU times: user 1d 12h 8min 51s, sys: 5min 12s, total: 1d 12h 14min 4s\n",
      "Wall time: 2h 30min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8161764705882353"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "# inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "inputs_list = []\n",
    "for i in range(60):\n",
    "    prompt = f\"\"\"Task: Compare the two sentences below and determine if they are paraphrases of each other. A paraphrase means that both sentences express the same idea, though they may use different words or structures.\n",
    "    Sentence 1: {dataset[\"train\"]['sentence1'][i]}\n",
    "    Sentence 2: {dataset[\"train\"]['sentence2'][i]}\n",
    "    Please only respond with \"yes\" if they are paraphrases, or \"no\" if they are not.\n",
    "    Answer:\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=256, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_mrpc(model, include_layers, ~any_mask, tokenizer, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e686553-0da8-4d09-b25e-5469b0fdedb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 114688)\n",
      "(1, 114688)\n",
      "[  12    4    5    3    4    1    3    1    1    2    3    2    5    2\n",
      "    4   13   53  497  479  861 1644 2570 2762 3523 3961 4090 4096 4096]\n",
      "28697\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49539821bc7940edae7901082d666537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8162\n",
      "CPU times: user 1d 17h 18min 55s, sys: 6min 1s, total: 1d 17h 24min 57s\n",
      "Wall time: 2h 53min 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8161764705882353"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "# inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "inputs_list = []\n",
    "for i in range(60):\n",
    "    prompt = f\"\"\"Task: Compare the two sentences below and determine if they are paraphrases of each other. A paraphrase means that both sentences express the same idea, though they may use different words or structures.\n",
    "    Sentence 1: {dataset[\"train\"]['sentence1'][i]}\n",
    "    Sentence 2: {dataset[\"train\"]['sentence2'][i]}\n",
    "    Please only respond with \"yes\" if they are paraphrases, or \"no\" if they are not.\n",
    "    Answer:\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=512, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_mrpc(model, include_layers, ~any_mask, tokenizer, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c5ae298-a024-48bd-83bb-abb31fa3911d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([414, 410, 429, 402, 404, 418, 417, 409, 401, 391, 413, 397, 448, 438,\n",
      "        404, 416, 426, 416, 398, 392, 456, 438, 423, 414, 396, 394, 380, 422])\n",
      "tensor(11566)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1523439/4174277738.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = torch.tensor(mask.reshape(28, 4096), dtype=bool)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba148063325e4011b40de4f5f4bcb99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.803921568627451"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_matrix = torch.rand(28, 4096) < 0.10\n",
    "print(mask_matrix.reshape(28, 4096).sum(axis=1))\n",
    "print(mask_matrix.sum())\n",
    "evaluate_masked_mrpc(model, include_layers, mask_matrix, tokenizer, dataset['validation'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89983730-1684-4403-976f-130ebe63f177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057419c9-c8c0-44ba-9854-5608ed8fddff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f8ef28d-8134-41e8-a1fc-756efc303829",
   "metadata": {},
   "source": [
    "## MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "879a9f65-2f52-4fda-ad92-991ca1a1781a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"nyu-mll/glue\", \"mnli\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c679afca-60a3-4160-b1f8-e35665cf44b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNLIDataset(Dataset):\n",
    "    def __init__(self, data_dict, tokenizer, max_length=300):\n",
    "        self.premise = data_dict['premise']\n",
    "        self.hypothesis = data_dict['hypothesis']\n",
    "        self.label = data_dict['label']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence1 = self.premise[idx]\n",
    "        sentence2 = self.hypothesis[idx]\n",
    "        label = self.label[idx]\n",
    "        \n",
    "        prompt = f\"\"\"Task: Determine the relationship between two sentences. The relationship can be one of three types: entailment, contradiction, or neutral.\n",
    "            Premise: {sentence1}\n",
    "            Hypothesis: {sentence2}\n",
    "            Please only answer with \"entailment\" if they are entailment, \"contradiction\" if they are contradiction, or \"neutral\" if they are neutral.\n",
    "            Answer: \"\"\"\n",
    " \n",
    "        encoding = self.tokenizer(prompt, return_tensors=\"pt\", max_length=self.max_length, truncation=True, padding=\"max_length\")\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': label\n",
    "        }\n",
    "        \n",
    "def predict_mnli(generated_answer):\n",
    "    if \"entailment\" in generated_answer.lower():\n",
    "        return 2\n",
    "    elif \"contradiction\" in generated_answer.lower():\n",
    "        return 0\n",
    "    elif \"neutral\" in generated_answer.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 3\n",
    "        \n",
    "def evaluate_mnli(model, include_layers, tokenizer, test_loader, batch_size=38):\n",
    "    test_dataset = MNLIDataset(test_loader, tokenizer, max_length=300)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = model.device\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = test_loader['label']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            generated_sequences = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=10\n",
    "            )\n",
    "            \n",
    "            generated_answers = []\n",
    "            for i in range(len(generated_sequences)):\n",
    "                input_length = input_ids.shape[1]\n",
    "                generated_answer = tokenizer.decode(\n",
    "                    generated_sequences[i][input_length:], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                generated_answers.append(generated_answer)\n",
    "            for i in range(len(generated_answers)):\n",
    "                generated_answer = generated_answers[i]\n",
    "                generated_answer = predict_mnli(generated_answer)\n",
    "                    \n",
    "                reference_answers = batch['label'][i]\n",
    "                all_predictions.append(generated_answer)\n",
    "    \n",
    "    acc = accuracy_score(all_predictions, all_labels)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    return acc\n",
    "\n",
    "def evaluate_masked_mnli(model, include_layers, mask, tokenizer, test_loader, batch_size=38):\n",
    "    test_dataset = MNLIDataset(test_loader, tokenizer, max_length=300)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    mask = torch.tensor(mask.reshape(28, 4096), dtype=bool)\n",
    "    \n",
    "    masked_model = MaskedModel(model, include_layers)\n",
    "    masked_model.register_hooks(mask)\n",
    "    \n",
    "    device = model.device\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = test_loader['label']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            generated_sequences = masked_model.forward(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=10\n",
    "            )\n",
    "            \n",
    "\n",
    "            generated_answers = []\n",
    "            for i in range(len(generated_sequences)):\n",
    "                input_length = input_ids.shape[1]\n",
    "                generated_answer = tokenizer.decode(\n",
    "                    generated_sequences[i][input_length:], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                generated_answers.append(generated_answer)\n",
    "            for i in range(len(generated_answers)):\n",
    "                generated_answer = generated_answers[i]\n",
    "                generated_answer = predict_mnli(generated_answer)\n",
    "                    \n",
    "                reference_answers = batch['label'][i]\n",
    "                all_predictions.append(generated_answer)\n",
    "                \n",
    "    masked_model.remove_hooks()\n",
    "    acc = accuracy_score(all_predictions, all_labels)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b26acd86-a454-496a-9eaf-b2a04e1ff374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e36c9f83a8349099f466120c95d876d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/289 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.20621497707590422"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_mnli(model, include_layers, tokenizer, dataset['validation_matched'], batch_size=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "278a443a-72b4-4f55-a96c-ca93f93bc1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 114688)\n",
      "(1, 114688)\n",
      "[   6    1    4    1    1    1    1    1    1    0    1    1    2    1\n",
      "    0    3    2    3    6    8   24   42  100  104  159  419  670 1837]\n",
      "3399\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ef456b06b64e83b3faedb3e905a59b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd4395bb7b44787a9a61b9cbb50c5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0000\n",
      "CPU times: user 2d 14h 26min 26s, sys: 12min 46s, total: 2d 14h 39min 12s\n",
      "Wall time: 8h 48min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "# inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "inputs_list = []\n",
    "for i in range(100):\n",
    "    prompt = f\"\"\"Task: Determine the relationship between two sentences. The relationship can be one of three types: entailment, contradiction, or neutral.\n",
    "            Premise: {dataset[\"train\"]['premise'][i]}\n",
    "            Hypothesis: {dataset[\"train\"]['hypothesis'][i]}\n",
    "            Please only answer with \"entailment\" if they are entailment, \"contradiction\" if they are contradiction, or \"neutral\" if they are neutral.\n",
    "            Answer: \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=10, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_mnli(model, include_layers, ~any_mask, tokenizer, dataset['validation_matched'], batch_size=32)\n",
    "evaluate_masked_mnli(model, include_layers, any_mask, tokenizer, dataset['validation_matched'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0347a14f-da69-49cc-9f70-4ae45ba12fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 114688)\n",
      "(1, 114688)\n",
      "[   7    2    5    1    3    1    1    1    1    1    1    1    5    2\n",
      "    2    3    3   13   12   27   92  213  474  789 1220 2491 3138 4060]\n",
      "12569\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3986d80fb37d45708c144d02b2d7a41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2123\n",
      "CPU times: user 2d 11h 1min 9s, sys: 11min 44s, total: 2d 11h 12min 54s\n",
      "Wall time: 6h 7min 51s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21232806928171166"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "# inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "inputs_list = []\n",
    "for i in range(100):\n",
    "    prompt = f\"\"\"Task: Determine the relationship between two sentences. The relationship can be one of three types: entailment, contradiction, or neutral.\n",
    "            Premise: {dataset[\"train\"]['premise'][i]}\n",
    "            Hypothesis: {dataset[\"train\"]['hypothesis'][i]}\n",
    "            Please only answer with \"entailment\" if they are entailment, \"contradiction\" if they are contradiction, or \"neutral\" if they are neutral.\n",
    "            Answer: \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=64, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_mnli(model, include_layers, ~any_mask, tokenizer, dataset['validation_matched'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dde4c59-df4a-4b59-9ea6-fc8cd6eb7552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 114688)\n",
      "(1, 114688)\n",
      "[  11    4    6    2    2    2    2    1    1    1    1    2    5    4\n",
      "    7    5    9   25   30   74  233  369  738 1226 1782 3190 3761 4093]\n",
      "15586\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b163274f7337430f926fcc12268ea45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2041\n",
      "CPU times: user 2d 10h 14min 40s, sys: 9min 23s, total: 2d 10h 24min 3s\n",
      "Wall time: 6h 2min 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.20407539480387163"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "# inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "inputs_list = []\n",
    "for i in range(100):\n",
    "    prompt = f\"\"\"Task: Determine the relationship between two sentences. The relationship can be one of three types: entailment, contradiction, or neutral.\n",
    "            Premise: {dataset[\"train\"]['premise'][i]}\n",
    "            Hypothesis: {dataset[\"train\"]['hypothesis'][i]}\n",
    "            Please only answer with \"entailment\" if they are entailment, \"contradiction\" if they are contradiction, or \"neutral\" if they are neutral.\n",
    "            Answer: \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=128, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_mnli(model, include_layers, ~any_mask, tokenizer, dataset['validation_matched'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47769aac-889e-4b64-93bf-918071d759bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 114688)\n",
      "(1, 114688)\n",
      "[  12    4    5    3    3    2    2    2    1    1    1    2    5    2\n",
      "    7   11   12   79   72  211  598  857 1413 2104 2651 3892 4055 4096]\n",
      "20103\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a435f21a5d945e8be7660297cc3aba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2062\n",
      "CPU times: user 2d 12h 4min 57s, sys: 9min 55s, total: 2d 12h 14min 52s\n",
      "Wall time: 6h 9min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.20621497707590422"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "# inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "inputs_list = []\n",
    "for i in range(100):\n",
    "    prompt = f\"\"\"Task: Determine the relationship between two sentences. The relationship can be one of three types: entailment, contradiction, or neutral.\n",
    "            Premise: {dataset[\"train\"]['premise'][i]}\n",
    "            Hypothesis: {dataset[\"train\"]['hypothesis'][i]}\n",
    "            Please only answer with \"entailment\" if they are entailment, \"contradiction\" if they are contradiction, or \"neutral\" if they are neutral.\n",
    "            Answer: \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=256, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_mnli(model, include_layers, ~any_mask, tokenizer, dataset['validation_matched'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0720c1d8-aa9f-4e8f-9e45-55095b002635",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "# inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "inputs_list = []\n",
    "for i in range(100):\n",
    "    prompt = f\"\"\"Task: Determine the relationship between two sentences. The relationship can be one of three types: entailment, contradiction, or neutral.\n",
    "            Premise: {dataset[\"train\"]['premise'][i]}\n",
    "            Hypothesis: {dataset[\"train\"]['hypothesis'][i]}\n",
    "            Please only answer with \"entailment\" if they are entailment, \"contradiction\" if they are contradiction, or \"neutral\" if they are neutral.\n",
    "            Answer: \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=512, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_mnli(model, include_layers, ~any_mask, tokenizer, dataset['validation_matched'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "388d2099-301e-4f5e-83dc-0a593143100c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([429, 430, 382, 418, 398, 364, 412, 413, 421, 366, 365, 425, 406, 397,\n",
      "        406, 404, 438, 430, 415, 423, 380, 380, 403, 399, 394, 366, 417, 404])\n",
      "tensor(11285)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1342043/2008069900.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = torch.tensor(mask.reshape(28, 4096), dtype=bool)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d5388ff6fa4bcbaa8201a6e3d2a003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22924095771777891"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_matrix = torch.rand(28, 4096) < 0.10\n",
    "print(mask_matrix.reshape(28, 4096).sum(axis=1))\n",
    "print(mask_matrix.sum())\n",
    "evaluate_masked_mnli(model, include_layers, mask_matrix, tokenizer, dataset['validation_matched'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f1b89-402d-4207-8d85-89609bb17e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd5529c4-ccd5-4703-bcf0-9d4c04ebc977",
   "metadata": {},
   "source": [
    "## QNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92eef866-1546-4dc7-abf7-a8054a630293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df3ea36584242398329ba90a8675984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc42e110adde4c589dfccb6da5b6f220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863b102b9353457eb45634729ea98f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/877k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4d255c4ed048589eb0946901174910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/104743 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5739d1965f464213973987e7ebf4f361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/5463 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573b023b33e148438b7a01c3e583561a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5463 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'sentence', 'label', 'idx'],\n",
       "        num_rows: 104743\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'sentence', 'label', 'idx'],\n",
       "        num_rows: 5463\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'sentence', 'label', 'idx'],\n",
       "        num_rows: 5463\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"nyu-mll/glue\", \"qnli\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df397ca5-7b84-4f16-a4c0-49651be3ac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNLIDataset(Dataset):\n",
    "    def __init__(self, data_dict, tokenizer, max_length=400):\n",
    "        self.question = data_dict['question']\n",
    "        self.sentence = data_dict['sentence']\n",
    "        self.label = data_dict['label']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence1 = self.question[idx]\n",
    "        sentence2 = self.sentence[idx]\n",
    "        label = self.label[idx]\n",
    "        \n",
    "        prompt = f\"\"\"Task: Analyze the following question and sentence to determine if the sentence contains the answer to the question. The relationship can be one of two types: entailment (the sentence answers the question) or not_entailment (the sentence does not answer the question).\n",
    "            Question: {sentence1}\n",
    "            Sentence: {sentence2}\n",
    "            Please only answer with \"Entailment\" if the sentence answers the question, or \"Not Entailment\" if it does not.\n",
    "            Answer:\"\"\"\n",
    " \n",
    "        encoding = self.tokenizer(prompt, return_tensors=\"pt\", max_length=self.max_length, truncation=True, padding=\"max_length\")\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': label\n",
    "        }\n",
    "        \n",
    "def predict_qnli(generated_answer):\n",
    "    if \"not entailment\" in generated_answer.lower():\n",
    "        return 0\n",
    "    elif \"entailment\" in generated_answer.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "        \n",
    "def evaluate_qnli(model, include_layers, tokenizer, test_loader, batch_size=38):\n",
    "    test_dataset = QNLIDataset(test_loader, tokenizer, max_length=400)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = model.device\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = test_loader['label']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            generated_sequences = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=10\n",
    "            )\n",
    "            \n",
    "            \n",
    "            generated_answers = []\n",
    "            for i in range(len(generated_sequences)):\n",
    "                input_length = input_ids.shape[1]\n",
    "                generated_answer = tokenizer.decode(\n",
    "                    generated_sequences[i][input_length:], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                generated_answers.append(generated_answer)\n",
    "            for i in range(len(generated_answers)):\n",
    "                generated_answer = generated_answers[i]\n",
    "                generated_answer = predict_qnli(generated_answer)\n",
    "                    \n",
    "                reference_answers = batch['label'][i]\n",
    "                all_predictions.append(generated_answer)\n",
    "    \n",
    "    acc = accuracy_score(all_predictions, all_labels)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    return acc\n",
    "\n",
    "def evaluate_masked_qnli(model, include_layers, mask, tokenizer, test_loader, batch_size=38):\n",
    "    test_dataset = QNLIDataset(test_loader, tokenizer, max_length=400)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    mask = torch.tensor(mask.reshape(28, 4096), dtype=bool)\n",
    "    \n",
    "    masked_model = MaskedModel(model, include_layers)\n",
    "    masked_model.register_hooks(mask)\n",
    "    \n",
    "    device = model.device\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = test_loader['label']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            generated_sequences = masked_model.forward(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=10\n",
    "            )\n",
    "            \n",
    "            \n",
    "            generated_answers = []\n",
    "            for i in range(len(generated_sequences)):\n",
    "                input_length = input_ids.shape[1]\n",
    "                generated_answer = tokenizer.decode(\n",
    "                    generated_sequences[i][input_length:], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                generated_answers.append(generated_answer)\n",
    "            for i in range(len(generated_answers)):\n",
    "                generated_answer = generated_answers[i]\n",
    "                generated_answer = predict_qnli(generated_answer)\n",
    "                    \n",
    "                reference_answers = batch['label'][i]\n",
    "                all_predictions.append(generated_answer)\n",
    "                \n",
    "    masked_model.remove_hooks()\n",
    "    acc = accuracy_score(all_predictions, all_labels)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ec25e7a-a118-4772-9092-9b52b3685624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2668a7d53b45c9923a16630b2dc76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10671792055647081"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_qnli(model, include_layers, tokenizer, dataset['validation'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3f72718-f050-4bf2-b4ae-1613f5092e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 114688)\n",
      "(1, 114688)\n",
      "[  8   3   7   1   2   0   1   1   1   1   1   1   5   2   2   3   3   4\n",
      "   3   5   6   5   8  21  32  65 164 550]\n",
      "905\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a210a352924a40819359b771318188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754d39174f8c4aec9d6a612eeb9dade2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0053\n",
      "CPU times: user 20h 44min 10s, sys: 2min 48s, total: 20h 46min 59s\n",
      "Wall time: 3h 35min 44s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.005308438586857038"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "# inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "inputs_list = []\n",
    "for i in range(100):\n",
    "    prompt = dataset['train']['question'][i] + dataset['train']['sentence'][i]\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=10, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_qnli(model, include_layers, ~any_mask, tokenizer, dataset['validation'], batch_size=32)\n",
    "evaluate_masked_qnli(model, include_layers, any_mask, tokenizer, dataset['validation'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22bd341d-87dd-48ce-aa40-d49e737e6cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 114688)\n",
      "(1, 114688)\n",
      "[   6    2    5    1    1    1    1    1    1    1    1    2    5    4\n",
      "    4    6    9   10  104  535  659  955 1272 1463 1563 1857 2398 2979]\n",
      "13846\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f9e1be0bb0467480d94219948b669a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1545\n",
      "CPU times: user 20h 22min 26s, sys: 2min 56s, total: 20h 25min 22s\n",
      "Wall time: 2h 23min 34s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15449386783818414"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "# inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "inputs_list = []\n",
    "for i in range(100):\n",
    "    prompt = dataset['train']['question'][i] + dataset['train']['sentence'][i]\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=64, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_qnli(model, include_layers, ~any_mask, tokenizer, dataset['validation'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9498b749-b0e5-4234-9a4b-70616ba2875d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 114688)\n",
      "(1, 114688)\n",
      "[   8    3    8    1    2    1    1    1    1    1    1    3    6    3\n",
      "    4    6    9   13  187  858 1288 1655 2312 2634 2726 3243 3597 3769]\n",
      "22341\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000ef44df3c84cccabbfdb5f4a03b817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1157\n",
      "CPU times: user 20h 30min 48s, sys: 2min 52s, total: 20h 33min 41s\n",
      "Wall time: 2h 23min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11568735127219476"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "# inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "inputs_list = []\n",
    "for i in range(100):\n",
    "    prompt = dataset['train']['question'][i] + dataset['train']['sentence'][i]\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=128, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_qnli(model, include_layers, ~any_mask, tokenizer, dataset['validation'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0a31eee-0164-4c14-a521-9445d25032aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 114688)\n",
      "(1, 114688)\n",
      "[   8    3    5    1    3    1    1    1    1    2    2    2    5    4\n",
      "    4    7   24   24  299 1140 1923 2545 3195 3526 3760 4011 4067 4081]\n",
      "28645\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca64bb183e8540faa041a5c318771600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1078\n",
      "CPU times: user 21h 56min 51s, sys: 2min 56s, total: 21h 59min 48s\n",
      "Wall time: 2h 28min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10781621819513088"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "# inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "inputs_list = []\n",
    "for i in range(100):\n",
    "    prompt = dataset['train']['question'][i] + dataset['train']['sentence'][i]\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=256, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_qnli(model, include_layers, ~any_mask, tokenizer, dataset['validation'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec0b3517-3dff-4758-b956-c5a24174e481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data3/liuyiheng/anaconda3/envs/torch/lib/python3.12/site-packages/sklearn/decomposition/_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 114688)\n",
      "(1, 114688)\n",
      "[  12    5    7    1    4    1    2    1    1    1    1    2    7    2\n",
      "    3   15   16   46  392 1253 2159 2996 3650 3927 4052 4090 4096 4096]\n",
      "30838\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc6fef4958d4e7e9b443bb2bedb7aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1067\n",
      "CPU times: user 1d 8h 40min 40s, sys: 3min 6s, total: 1d 8h 43min 46s\n",
      "Wall time: 3h 2min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10671792055647081"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "# inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "inputs_list = []\n",
    "for i in range(100):\n",
    "    prompt = dataset['train']['question'][i] + dataset['train']['sentence'][i]\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=512, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_qnli(model, include_layers, ~any_mask, tokenizer, dataset['validation'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "344a547d-bbc4-4e40-bcf6-fe48fe791909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([412, 409, 420, 422, 435, 418, 398, 424, 415, 407, 401, 385, 403, 406,\n",
      "        386, 414, 419, 418, 434, 416, 397, 420, 400, 424, 422, 425, 437, 396])\n",
      "tensor(11563)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1342043/1930238430.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = torch.tensor(mask.reshape(28, 4096), dtype=bool)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb40b8a8fa349bbaab9debd1100f730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11971444261394838"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_matrix = torch.rand(28, 4096) < 0.10\n",
    "print(mask_matrix.reshape(28, 4096).sum(axis=1))\n",
    "print(mask_matrix.sum())\n",
    "evaluate_masked_qnli(model, include_layers, mask_matrix, tokenizer, dataset['validation'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad2878-702c-4997-a911-fb058e086ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b97a6e5-a7ab-4595-b452-e356bea3458c",
   "metadata": {},
   "source": [
    "## AGNEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3ab6597-38a9-4841-a24d-48b86d463603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de090d63511e4ffb9a21b5ad0cacb1ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a001ead3a64149428a526866b8a73c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d062dfee2e489aacbe08351f33a8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e31e4b371446a6b68592c0c57265f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473d00b890744385ac628b6b9817aece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 7600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('fancyzhx/ag_news')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3fc0f06e-2137-4bfb-ad94-36dbe8f67b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGNEWSDataset(Dataset):\n",
    "    def __init__(self, data_dict, tokenizer, max_length=1024):\n",
    "        self.texts = data_dict['text']\n",
    "        self.labels = data_dict['label']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        prompt = f\"\"\"Task: Classify the following news article into one of four categories: World, Sports, Business, or Sci/Tech.\n",
    "        Article: {text}\n",
    "        You only need to answer the article belongs to the [World, Sports, Business or Sci/Tech] category.\n",
    "        Answer: \"\"\"\n",
    "        encoding = self.tokenizer(prompt, return_tensors=\"pt\", max_length=self.max_length, truncation=True, padding=\"max_length\")\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(label)\n",
    "        }\n",
    "def predict_agnews(generated_answer):\n",
    "    if \"world\" in generated_answer.lower():\n",
    "        return 0\n",
    "    elif \"sports\" in generated_answer.lower():\n",
    "        return 1\n",
    "    elif \"business\" in generated_answer.lower():\n",
    "        return 2\n",
    "    elif \"sci/tech\" in generated_answer.lower():\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "        \n",
    "def evaluate_agnews(model, include_layers, tokenizer, test_loader, batch_size=38):\n",
    "    test_dataset = AGNEWSDataset(test_loader, tokenizer, max_length=400)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = model.device\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = test_loader['label']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            generated_sequences = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=10\n",
    "            )\n",
    "            \n",
    "            \n",
    "            generated_answers = []\n",
    "            for i in range(len(generated_sequences)):\n",
    "                input_length = input_ids.shape[1]\n",
    "                generated_answer = tokenizer.decode(\n",
    "                    generated_sequences[i][input_length:], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                generated_answers.append(generated_answer)\n",
    "            for i in range(len(generated_answers)):\n",
    "                generated_answer = generated_answers[i]\n",
    "                generated_answer = predict_agnews(generated_answer)\n",
    "                    \n",
    "                reference_answers = batch['label'][i]\n",
    "                all_predictions.append(generated_answer)\n",
    "    \n",
    "    acc = accuracy_score(all_predictions, all_labels)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    return acc\n",
    "    \n",
    "def evaluate_masked_agnews(model, include_layers, mask, tokenizer, test_loader, batch_size=38):\n",
    "    test_dataset = AGNEWSDataset(test_loader, tokenizer, max_length=400)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    mask = torch.tensor(mask.reshape(28, 4096), dtype=bool)\n",
    "    \n",
    "    masked_model = MaskedModel(model, include_layers)\n",
    "    masked_model.register_hooks(mask)\n",
    "    \n",
    "    device = model.device\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = test_loader['label']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            generated_sequences = masked_model.forward(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=10\n",
    "            )\n",
    "            \n",
    "            \n",
    "            generated_answers = []\n",
    "            for i in range(len(generated_sequences)):\n",
    "                input_length = input_ids.shape[1]\n",
    "                generated_answer = tokenizer.decode(\n",
    "                    generated_sequences[i][input_length:], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                generated_answers.append(generated_answer)\n",
    "            for i in range(len(generated_answers)):\n",
    "                generated_answer = generated_answers[i]\n",
    "                generated_answer = predict_agnews(generated_answer)\n",
    "                    \n",
    "                reference_answers = batch['label'][i]\n",
    "                all_predictions.append(generated_answer)\n",
    "                \n",
    "    masked_model.remove_hooks()\n",
    "    acc = accuracy_score(all_predictions, all_labels)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a79b576e-038d-4014-871b-9e01de472a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e414cb87374d7e93775c0041af7b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9127631578947368"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_agnews(model, include_layers, tokenizer, dataset['test'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "418e61b8-be28-4394-90bc-1efa6a79a33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 114688)\n",
      "(1, 114688)\n",
      "[  9   1   5   1   2   1   1   1   1   0   1   1   2   2   2   5   4   6\n",
      "   7  12  27  41  45  29  77 315 586 914]\n",
      "2098\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726f52d1331c4315bd4dc208a7cd8ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1add2ca5c02d4676bee8094b58f1ed1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0025\n",
      "CPU times: user 2d 8h 5min 6s, sys: 6min 3s, total: 2d 8h 11min 10s\n",
      "Wall time: 6h 32min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0025"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "# inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "inputs_list = []\n",
    "for i in range(100):\n",
    "    prompt = f\"\"\"Task: Classify the following news article into one of four categories: World, Sports, Business, or Sci/Tech.\n",
    "        Article: {dataset['train']['text'][i]}\n",
    "        You only need to answer the article belongs to the [World, Sports, Business or Sci/Tech] category.\n",
    "        Answer: \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=10, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_agnews(model, include_layers, ~any_mask, tokenizer, dataset['test'], batch_size=32)\n",
    "evaluate_masked_agnews(model, include_layers, any_mask, tokenizer, dataset['test'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "075ac204-d202-4cae-9130-586c68f50f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 114688)\n",
      "(1, 114688)\n",
      "[   8    2    4    1    3    2    2    1    1    1    1    2    5    3\n",
      "    3   15   36   96   88  189  284  585  436  648  947 2161 2874 3433]\n",
      "11831\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb51fdd783d141a5b7f55ac0a8a5b968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9036\n",
      "CPU times: user 2d 7h 5min 31s, sys: 5min 51s, total: 2d 7h 11min 22s\n",
      "Wall time: 4h 59min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9035526315789474"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "# inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "inputs_list = []\n",
    "for i in range(100):\n",
    "    prompt = f\"\"\"Task: Classify the following news article into one of four categories: World, Sports, Business, or Sci/Tech.\n",
    "        Article: {dataset['train']['text'][i]}\n",
    "        You only need to answer the article belongs to the [World, Sports, Business or Sci/Tech] category.\n",
    "        Answer: \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=64, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_agnews(model, include_layers, ~any_mask, tokenizer, dataset['test'], batch_size=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b952e4d-f156-4d88-a4a7-14c8300dc362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 114688)\n",
      "(1, 114688)\n",
      "[  10    3    8    2    2    1    1    1    1    1    2    2    5    3\n",
      "    5   18   62  161  291  511  972 1228 1190 1453 1758 3220 3687 3942]\n",
      "18540\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b64baf09a645d98a198046091077ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9087\n",
      "CPU times: user 2d 7h 33min 3s, sys: 4min 54s, total: 2d 7h 37min 57s\n",
      "Wall time: 4h 48min 15s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9086842105263158"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "# inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "inputs_list = []\n",
    "for i in range(100):\n",
    "    prompt = f\"\"\"Task: Classify the following news article into one of four categories: World, Sports, Business, or Sci/Tech.\n",
    "        Article: {dataset['train']['text'][i]}\n",
    "        You only need to answer the article belongs to the [World, Sports, Business or Sci/Tech] category.\n",
    "        Answer: \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=128, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_agnews(model, include_layers, ~any_mask, tokenizer, dataset['test'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a5da827-f94f-4965-b4cf-63b57adfd7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 114688)\n",
      "(1, 114688)\n",
      "[   8    3    7    2    3    1    1    1    1    1    1    2    5    3\n",
      "    6   27   56  205  492 1134 1884 2282 2375 2820 3136 3893 4056 4084]\n",
      "26489\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1f148daf6d48b3b6dbd38d81abeb10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9128\n",
      "CPU times: user 2d 7h 33min 23s, sys: 4min 57s, total: 2d 7h 38min 21s\n",
      "Wall time: 4h 47min 24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9127631578947368"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "# inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "inputs_list = []\n",
    "for i in range(100):\n",
    "    prompt = f\"\"\"Task: Classify the following news article into one of four categories: World, Sports, Business, or Sci/Tech.\n",
    "        Article: {dataset['train']['text'][i]}\n",
    "        You only need to answer the article belongs to the [World, Sports, Business or Sci/Tech] category.\n",
    "        Answer: \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=256, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_agnews(model, include_layers, ~any_mask, tokenizer, dataset['test'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1435169d-1a8f-42e2-b2d2-3bf61e241bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 114688)\n",
      "(1, 114688)\n",
      "[  11    4    6    2    2    2    2    1    1    1    1    2    5    3\n",
      "    6   27   79  336  905 1782 2762 3298 3542 3832 3968 4092 4096 4096]\n",
      "32864\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a650c845cd374ffcba9c55dcc7cb7480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9128\n",
      "CPU times: user 2d 17h 30min 24s, sys: 5min 25s, total: 2d 17h 35min 50s\n",
      "Wall time: 5h 24min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9127631578947368"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "# inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "inputs_list = []\n",
    "for i in range(100):\n",
    "    prompt = f\"\"\"Task: Classify the following news article into one of four categories: World, Sports, Business, or Sci/Tech.\n",
    "        Article: {dataset['train']['text'][i]}\n",
    "        You only need to answer the article belongs to the [World, Sports, Business or Sci/Tech] category.\n",
    "        Answer: \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=512, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_agnews(model, include_layers, ~any_mask, tokenizer, dataset['test'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4909cd4-284f-4211-9c5a-f870115d4bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([412, 410, 352, 421, 419, 407, 424, 417, 387, 434, 404, 415, 420, 439,\n",
      "        411, 382, 409, 434, 409, 385, 393, 428, 415, 389, 391, 411, 411, 413])\n",
      "tensor(11442)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1342043/2180117371.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = torch.tensor(mask.reshape(28, 4096), dtype=bool)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ec7775176c4e7faf6fc0eaae7a61be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9030263157894737"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_matrix = torch.rand(28, 4096) < 0.10\n",
    "print(mask_matrix.reshape(28, 4096).sum(axis=1))\n",
    "print(mask_matrix.sum())\n",
    "evaluate_masked_agnews(model, include_layers, mask_matrix, tokenizer, dataset['test'], batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b44ee6f-4e61-4110-ad8f-f019ae0cd692",
   "metadata": {},
   "source": [
    "## qqp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28084c3c-e16c-4447-80ac-990fb6cabb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4e94e926fa412d8032d0dadddab6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/33.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5306473679d4b97b17abf53d9b29078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/3.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961fb3287bbf458789bcde7333577033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/36.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47629ff56550410d9e0fa171cf893291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/363846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2c5da906534fb4a02d2bfdee738e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/40430 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0dd0f86df694f5c9968dc78c4fd4d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/390965 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question1', 'question2', 'label', 'idx'],\n",
       "        num_rows: 363846\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question1', 'question2', 'label', 'idx'],\n",
       "        num_rows: 40430\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question1', 'question2', 'label', 'idx'],\n",
       "        num_rows: 390965\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"nyu-mll/glue\", \"qqp\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1757a004-f48a-4f1c-bb6f-7f5e20b02d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the top ten Consumer-to-Consumer E-commerce online?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test']['question1'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9c7929d-aa84-453c-b701-6392938fd6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QQPDataset(Dataset):\n",
    "    def __init__(self, data_dict, tokenizer, max_length=1024):\n",
    "        self.question1 = data_dict['question1']\n",
    "        self.question2 = data_dict['question2']\n",
    "        self.labels = data_dict['label']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question1 = self.question1[idx]\n",
    "        question2 = self.question2[idx]\n",
    "        label = self.labels[idx]\n",
    "        prompt = f\"\"\"Task: Compare the following two questions, paying attention to their topics, details, and intents. Then determine whether they are asking the same thing.\n",
    "                Question 1: {question1}\n",
    "                Question 2: {question2}\n",
    "                Please answer \"same\" if they are asking the same thing, or \"different\" if they are not. Answer:\"\"\"\n",
    "        encoding = self.tokenizer(prompt, return_tensors=\"pt\", max_length=self.max_length, truncation=True, padding=\"max_length\")\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(label)\n",
    "        }\n",
    "def predict_qqp(generated_answer):\n",
    "    if \"different\" in generated_answer.lower():\n",
    "        return 0\n",
    "    elif \"same\" in generated_answer.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "        \n",
    "def evaluate_qqp(model, include_layers, tokenizer, test_loader, batch_size=38):\n",
    "    test_dataset = QQPDataset(test_loader, tokenizer, max_length=400)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    device = model.device\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = test_loader['label']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            generated_sequences = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=10\n",
    "            )\n",
    "            \n",
    "            \n",
    "            generated_answers = []\n",
    "            for i in range(len(generated_sequences)):\n",
    "                input_length = input_ids.shape[1]\n",
    "                generated_answer = tokenizer.decode(\n",
    "                    generated_sequences[i][input_length:], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                generated_answers.append(generated_answer)\n",
    "            for i in range(len(generated_answers)):\n",
    "                generated_answer = generated_answers[i]\n",
    "                generated_answer = predict_qqp(generated_answer)\n",
    "                    \n",
    "                reference_answers = batch['label'][i]\n",
    "                all_predictions.append(generated_answer)\n",
    "    \n",
    "    acc = accuracy_score(all_predictions, all_labels)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    return acc\n",
    "    \n",
    "def evaluate_masked_qqp(model, include_layers, mask, tokenizer, test_loader, batch_size=38):\n",
    "    test_dataset = QQPDataset(test_loader, tokenizer, max_length=400)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    mask = torch.tensor(mask.reshape(28, 4096), dtype=bool)\n",
    "    \n",
    "    masked_model = MaskedModel(model, include_layers)\n",
    "    masked_model.register_hooks(mask)\n",
    "    \n",
    "    device = model.device\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = test_loader['label']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            generated_sequences = masked_model.forward(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=10\n",
    "            )\n",
    "            \n",
    "            \n",
    "            generated_answers = []\n",
    "            for i in range(len(generated_sequences)):\n",
    "                input_length = input_ids.shape[1]\n",
    "                generated_answer = tokenizer.decode(\n",
    "                    generated_sequences[i][input_length:], \n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                generated_answers.append(generated_answer)\n",
    "            for i in range(len(generated_answers)):\n",
    "                generated_answer = generated_answers[i]\n",
    "                generated_answer = predict_qqp(generated_answer)\n",
    "                    \n",
    "                reference_answers = batch['label'][i]\n",
    "                all_predictions.append(generated_answer)\n",
    "                \n",
    "    masked_model.remove_hooks()\n",
    "    acc = accuracy_score(all_predictions, all_labels)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2487505d-748e-4eba-aa93-9679d5fc28fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210b152ed00c4b75809fbc71fdedc27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/963 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_qqp(model, include_layers, tokenizer, dataset['validation'], batch_size=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1a7bc1-cdfc-4f5b-9d5f-85355d4ee83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "extractor = GroupFBNFeatureExtractor(model, include_layers=include_layers, device=model.device)\n",
    "# inputs_list = [tokenizer(f\"Task: Determine whether the following English sentences are grammatically correct or linguistically acceptable. Please only answer 'acceptable' or 'unacceptable'. \\n Sentence:{inputs} \\n Answer:\", return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset['train']['sentence'][:200]]\n",
    "inputs_list = []\n",
    "for i in range(50):\n",
    "    prompt = dataset['train']['question1'][i] + \"\\t\" + dataset['train']['question2'][i]\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs_list.append(inputs)\n",
    "feature_masked = extractor.fit(inputs_list=inputs_list, n_components=10, alpha=alpha, random_state=666)\n",
    "components = extractor.mixing_\n",
    "print(components.shape)\n",
    "any_mask = np.any(components, axis=0).reshape(1, -1)\n",
    "print(any_mask.shape)\n",
    "print(any_mask.reshape(28, 4096).sum(axis=1))\n",
    "print(any_mask.sum())\n",
    "evaluate_masked_qqp(model, include_layers, ~any_mask, tokenizer, dataset['test'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88676f04-c324-455a-95c4-2b4f7969040f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
