{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "476ae0ae-19e9-41ad-b02c-0d2e1bae4412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmfact import LayerOutputExtractor, FBNFeatureExtractor, GroupFBNFeatureExtractor, FBNExtractor, LLMFC\n",
    "from llmfact.extractor import MutiLayerAnalysis, MutiLayerAnalysis2\n",
    "from llmfact.extractor import SingleLayerAnalysis\n",
    "from llmfact.mask import MaskedGPT2ForSequenceClassification, MaskedGPT2AmplifiedForSequenceClassification, MaskedGPT2LMModel, MaskedModel\n",
    "from transformers import GPT2Model, GPT2Config, GPT2LMHeadModel, GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, AutoModelForCausalLM, AutoModelForQuestionAnswering\n",
    "from datasets import load_dataset, get_dataset_config_names, get_dataset_split_names\n",
    "from torch.utils.data import DataLoader\n",
    "# from rouge_score import rouge_scorer\n",
    "from evaluate import load\n",
    "\n",
    "from llmfact.utils import IoU, correlation_activation, thresholding, write_layer_txt, evaluate_iou\n",
    "from llmfact.pruner.llama import LayerBiasCompute\n",
    "from llmfact.stat import  StatICA, StatDictionaryLearning\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import FastICA\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4,5,6'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]  = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "153884d0-e452-4d45-b72a-f047c3be9c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7275a53023fe43ecbd245825193dc29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"lmsys/vicuna-7b-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46dbbbf2-02c3-4796-8418-b599fdd22c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.layers.0.mlp.down_proj',\n",
       " 'model.layers.1.mlp.down_proj',\n",
       " 'model.layers.2.mlp.down_proj',\n",
       " 'model.layers.3.mlp.down_proj',\n",
       " 'model.layers.4.mlp.down_proj',\n",
       " 'model.layers.5.mlp.down_proj',\n",
       " 'model.layers.6.mlp.down_proj',\n",
       " 'model.layers.7.mlp.down_proj',\n",
       " 'model.layers.8.mlp.down_proj',\n",
       " 'model.layers.9.mlp.down_proj',\n",
       " 'model.layers.10.mlp.down_proj',\n",
       " 'model.layers.11.mlp.down_proj',\n",
       " 'model.layers.12.mlp.down_proj',\n",
       " 'model.layers.13.mlp.down_proj',\n",
       " 'model.layers.14.mlp.down_proj',\n",
       " 'model.layers.15.mlp.down_proj',\n",
       " 'model.layers.16.mlp.down_proj',\n",
       " 'model.layers.17.mlp.down_proj',\n",
       " 'model.layers.18.mlp.down_proj',\n",
       " 'model.layers.19.mlp.down_proj',\n",
       " 'model.layers.20.mlp.down_proj',\n",
       " 'model.layers.21.mlp.down_proj',\n",
       " 'model.layers.22.mlp.down_proj',\n",
       " 'model.layers.23.mlp.down_proj',\n",
       " 'model.layers.24.mlp.down_proj',\n",
       " 'model.layers.25.mlp.down_proj',\n",
       " 'model.layers.26.mlp.down_proj',\n",
       " 'model.layers.27.mlp.down_proj',\n",
       " 'model.layers.28.mlp.down_proj',\n",
       " 'model.layers.29.mlp.down_proj',\n",
       " 'model.layers.30.mlp.down_proj',\n",
       " 'model.layers.31.mlp.down_proj']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include_layers = []\n",
    "for name, _ in model.named_modules():\n",
    "    if \"mlp.down\" in name:\n",
    "        include_layers.append(name)\n",
    "include_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "778e5eea-cda0-48fb-a9b7-8196bc41e8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "808b286c-fe60-4926-9d42-ad7908e2e3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 15313\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "wiki_dataset = load_dataset(\"Self-GRIT/wikitext-2-raw-v1-preprocessed\", split='train')\n",
    "print(wiki_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6896e22-82bf-4683-8446-02c7f77ef87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9984, 704512)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_components = np.load(\"./data/FBN/text3200-mlp.act-CanICA-SingleICA-max_iter-300_vicuna-7b-v1.5-muti-layer-wise_128_normal_mixing_std_True.npy\")\n",
    "normal_components.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2435222-e3cb-46fe-9f72-c500ab44de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_par_num(neuron_num_list):\n",
    "    total_par = 6738415616\n",
    "    print(\"total parameters:\", total_par)\n",
    "    \n",
    "    total_mlp = 32 * (4096 * 11008 * 3)\n",
    "    print(\"total mlp parameters:\", total_mlp)\n",
    "\n",
    "    total_cut = 0\n",
    "    for i in neuron_num_list:\n",
    "        cut_num = 4096 * 11008 * 3 - (i * 4096 * 3)\n",
    "        total_cut += cut_num\n",
    "    print(\"total cut parameters num:\", total_cut)\n",
    "\n",
    "    print(f\"total cut mlp parameters: {total_cut / total_mlp:.4f}\")\n",
    "    print(f\"total cut parameters: {total_cut / total_par:.4f}\")\n",
    "    print(f\"parameters after cut: {total_par - total_cut:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40eda0ce-a1bc-4f03-aa20-34789c0613a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283466\n",
      "247204.0\n",
      "[11008. 11008. 11008.  4913.  5599.  6116.  6687.  7346.  7303.  7405.\n",
      "  7558.  7618.  7614.  7685.  7729.  7668.  8015.  7769.  7727.  7345.\n",
      "  7426.  7331.  7481.  7237.  7329.  6992.  6733.  6717.  6400.  6421.\n",
      " 11008. 11008.]\n",
      "total parameters: 6738415616\n",
      "total mlp parameters: 4328521728\n",
      "total cut parameters num: 1290878976.0\n",
      "total cut mlp parameters: 0.2982\n",
      "total cut parameters: 0.1916\n",
      "parameters after cut: 5447536640.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 11008)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any_mask = np.abs(normal_components) > 5.98\n",
    "any_mask = np.any(any_mask, axis=0).reshape(1, -1)\n",
    "print(any_mask.sum())\n",
    "\n",
    "mask = any_mask.reshape(32, 2, -1)\n",
    "mask_matrix = np.ones((32, 11008))\n",
    "for i in range(3, mask.shape[0] - 2):\n",
    "    mask_matrix[i] = np.any(mask[i], axis=0)\n",
    "print(mask_matrix.sum())\n",
    "print(mask_matrix.sum(axis=1))\n",
    "cut_par_num(mask_matrix.sum(axis=1))\n",
    "# mask_matrix = np.repeat(mask_matrix, 2, axis=0)\n",
    "mask_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03a629aa-5b06-43c5-b7fb-a73053df5203",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_matrix = np.array(mask_matrix, dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "708b2f50-39c6-4e13-8de7-c4ea188d370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hooks(module):\n",
    "    if hasattr(module, \"_forward_hooks\"):\n",
    "        module._forward_hooks.clear()\n",
    "    if hasattr(module, \"_backward_hooks\"):\n",
    "        module._backward_hooks.clear()\n",
    "    for child in module.children():\n",
    "        remove_hooks(child)\n",
    "\n",
    "# 调用此函数以清理整个模型的钩子\n",
    "remove_hooks(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b9c03db-cc12-4c82-955c-518f90bdfdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del normal_components\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "082d3684-271e-4a2c-96de-7b3cedb381d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "370e5336-d6ec-433f-9fde-23e9fd4ccbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(247204)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_matrix.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0d23060-3851-4f49-a237-86b6619bf67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./data/FBN/vicuna-7B-mask_0.2_wiki_2000+_threshold_5.98.npy\", mask_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af8cac28-7169-48cf-aae4-4875c6fb4390",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_matrix = np.load(\"./data/FBN/vicuna-7B-mask_0.2_wiki_2000+_threshold_5.98.npy\")\n",
    "mask_matrix = np.array(mask_matrix, dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "490b3a34-6e99-411e-94c7-b0a7ea837980",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [34:31<00:00,  1.45it/s]\n"
     ]
    }
   ],
   "source": [
    "remove_hooks(model)\n",
    "add_bias = LayerBiasCompute(model, include_layers, tokenizer, ~mask_matrix, wiki_dataset['text'][:3000], 32)\n",
    "add_bias.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01e8893c-38e9-4e83-985f-b0b4bd5256b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1001,  0.1137,  0.0005,  ..., -0.0333,  0.0772,  0.0722],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_bias.bias_dict[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7384d16c-f235-419a-ad23-53165ea4ce6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.1001,  0.1137,  0.0005,  ..., -0.0333,  0.0772,  0.0722],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_bias.model.model.layers[6].mlp.down_proj.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fed8fce-9562-42cd-8638-2e4316588b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=True)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ef1bc5c-f53d-42be-bf44-42a8b4b8633f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters before pruned: 6738546688\n",
      "total parameters after pruned: 5447536640\n",
      "total cut num: 1291010048\n",
      "pruned rate: 0.1916\n"
     ]
    }
   ],
   "source": [
    "from llmfact.pruner.pruner import PrunedLlamaModel\n",
    "pruner = PrunedLlamaModel(model, mask_matrix)\n",
    "model = pruner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cad61a56-b810-49b6-9229-e3fd6b6f5bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    }
   ],
   "source": [
    "from lm_eval import evaluator\n",
    "import lm_eval\n",
    "wrapper_model = lm_eval.models.huggingface.HFLM(pretrained=model, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68fd4de9-036a-40cf-ab1f-8fca027e211e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\n",
      "[Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\n",
      "[Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\n",
      "[Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\n",
      "[Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte\n",
      "[Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False\n",
      "Overwriting default num_fewshot of wikitext from None to 0\n",
      "100%|██████████| 62/62 [00:00<00:00, 399.36it/s]\n",
      "  0%|          | 0/62 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (5943 > 4096). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 62/62 [00:01<00:00, 53.18it/s]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:02<00:00,  2.23s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.16s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.16s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:04<00:00,  4.69s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.15s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.16s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:03<00:00,  3.89s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:04<00:00,  4.77s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.15s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.16s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.14s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:04<00:00,  4.54s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.21s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.20s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:02<00:00,  2.91s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:05<00:00,  5.70s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.76s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:04<00:00,  4.11s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:03<00:00,  3.13s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.20s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.22s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.14s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.81s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.15s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.16s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.24s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.21s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:02<00:00,  2.29s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.25s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:04<00:00,  4.57s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.16s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.16s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.15s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.16s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.20s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.21s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.96s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  8.52it/s]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:02<00:00,  2.76s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.16s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:03<00:00,  3.11s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.21s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.27s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.16s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:02<00:00,  2.86s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.24s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.16s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.21s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.21s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.21s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:04<00:00,  4.52s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:04<00:00,  4.13s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.94s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:02<00:00,  2.17s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.14s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.21s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:02<00:00,  2.47s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:02<00:00,  2.73s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:05<00:00,  5.64s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:05<00:00,  5.31s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.94s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.20s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.20s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.16s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.16s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.21s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.15s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'wikitext': {'alias': 'wikitext',\n",
       "  'word_perplexity,none': 16.366720485858323,\n",
       "  'word_perplexity_stderr,none': 'N/A',\n",
       "  'byte_perplexity,none': 1.6866186555790748,\n",
       "  'byte_perplexity_stderr,none': 'N/A',\n",
       "  'bits_per_byte,none': 0.7541338171961662,\n",
       "  'bits_per_byte_stderr,none': 'N/A'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluator.simple_evaluate( \n",
    "    model=wrapper_model,\n",
    "    model_args=\"lmsys/vicuna-7b-v1.5\",\n",
    "    tasks=[\"wikitext\"],\n",
    "    num_fewshot=0,\n",
    "    task_manager=lm_eval.tasks.TaskManager(),\n",
    "    batch_size=1)\n",
    "results['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96ceb0b2-abd6-44b3-80f6-d4700b35a837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwriting default num_fewshot of arc_challenge from None to 0\n",
      "Overwriting default num_fewshot of arc_easy from None to 0\n",
      "Overwriting default num_fewshot of openbookqa from None to 0\n",
      "Overwriting default num_fewshot of winogrande from None to 0\n",
      "Overwriting default num_fewshot of hellaswag from None to 0\n",
      "Overwriting default num_fewshot of piqa from None to 0\n",
      "100%|██████████| 1172/1172 [00:01<00:00, 829.03it/s]\n",
      "100%|██████████| 2376/2376 [00:02<00:00, 817.85it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1662.19it/s]\n",
      "100%|██████████| 1267/1267 [00:00<00:00, 85286.20it/s]\n",
      "100%|██████████| 10042/10042 [00:07<00:00, 1355.82it/s]\n",
      "100%|██████████| 1838/1838 [00:03<00:00, 471.59it/s]\n",
      "Running loglikelihood requests: 100%|██████████| 62566/62566 [4:02:00<00:00,  4.31it/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'arc_challenge': {'alias': 'arc_challenge',\n",
       "  'acc,none': 0.36689419795221845,\n",
       "  'acc_stderr,none': 0.014084133118104419,\n",
       "  'acc_norm,none': 0.39334470989761094,\n",
       "  'acc_norm_stderr,none': 0.014275101465692932},\n",
       " 'arc_easy': {'alias': 'arc_easy',\n",
       "  'acc,none': 0.6839225589225589,\n",
       "  'acc_stderr,none': 0.009540440071928223,\n",
       "  'acc_norm,none': 0.6342592592592593,\n",
       "  'acc_norm_stderr,none': 0.009882988069418874},\n",
       " 'hellaswag': {'alias': 'hellaswag',\n",
       "  'acc,none': 0.49741087432782316,\n",
       "  'acc_stderr,none': 0.004989714512282005,\n",
       "  'acc_norm,none': 0.6650069707229636,\n",
       "  'acc_norm_stderr,none': 0.004710234188047076},\n",
       " 'openbookqa': {'alias': 'openbookqa',\n",
       "  'acc,none': 0.306,\n",
       "  'acc_stderr,none': 0.020629569998345414,\n",
       "  'acc_norm,none': 0.392,\n",
       "  'acc_norm_stderr,none': 0.02185468495561119},\n",
       " 'piqa': {'alias': 'piqa',\n",
       "  'acc,none': 0.7301414581066377,\n",
       "  'acc_stderr,none': 0.010356595421852079,\n",
       "  'acc_norm,none': 0.735038084874864,\n",
       "  'acc_norm_stderr,none': 0.010296557993316033},\n",
       " 'winogrande': {'alias': 'winogrande',\n",
       "  'acc,none': 0.6669297553275454,\n",
       "  'acc_stderr,none': 0.01324619402807062}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluator.simple_evaluate( \n",
    "    model=wrapper_model,\n",
    "    tasks=[\"piqa\", \"hellaswag\", \"winogrande\", \"openbookqa\", \"arc_easy\", \"arc_challenge\"],\n",
    "    num_fewshot=0,\n",
    "    task_manager=lm_eval.tasks.TaskManager(),\n",
    "    batch_size=1)\n",
    "results['results']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
