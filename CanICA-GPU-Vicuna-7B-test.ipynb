{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "476ae0ae-19e9-41ad-b02c-0d2e1bae4412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmfact import LayerOutputExtractor, FBNFeatureExtractor, GroupFBNFeatureExtractor, FBNExtractor, LLMFC\n",
    "from llmfact.extractor import MutiLayerAnalysis, MutiLayerAnalysis2\n",
    "from llmfact.extractor import SingleLayerAnalysis\n",
    "from llmfact.mask import MaskedGPT2ForSequenceClassification, MaskedGPT2AmplifiedForSequenceClassification, MaskedGPT2LMModel, MaskedModel\n",
    "from transformers import GPT2Model, GPT2Config, GPT2LMHeadModel, GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, AutoModelForCausalLM, AutoModelForQuestionAnswering\n",
    "from datasets import load_dataset, get_dataset_config_names, get_dataset_split_names\n",
    "from torch.utils.data import DataLoader\n",
    "# from rouge_score import rouge_scorer\n",
    "from evaluate import load\n",
    "\n",
    "from llmfact.utils import IoU, correlation_activation, thresholding, write_layer_txt, evaluate_iou\n",
    "from llmfact.stat import  StatICA, StatDictionaryLearning\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import FastICA\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,5,6,7,8,9'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]  = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "153884d0-e452-4d45-b72a-f047c3be9c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71e80ab39d241c8aacb2dde5c1c0de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"lmsys/vicuna-7b-v1.5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46dbbbf2-02c3-4796-8418-b599fdd22c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.layers.0.mlp.up_proj',\n",
       " 'model.layers.0.mlp.act_fn',\n",
       " 'model.layers.1.mlp.up_proj',\n",
       " 'model.layers.1.mlp.act_fn',\n",
       " 'model.layers.2.mlp.up_proj',\n",
       " 'model.layers.2.mlp.act_fn',\n",
       " 'model.layers.3.mlp.up_proj',\n",
       " 'model.layers.3.mlp.act_fn',\n",
       " 'model.layers.4.mlp.up_proj',\n",
       " 'model.layers.4.mlp.act_fn',\n",
       " 'model.layers.5.mlp.up_proj',\n",
       " 'model.layers.5.mlp.act_fn',\n",
       " 'model.layers.6.mlp.up_proj',\n",
       " 'model.layers.6.mlp.act_fn',\n",
       " 'model.layers.7.mlp.up_proj',\n",
       " 'model.layers.7.mlp.act_fn',\n",
       " 'model.layers.8.mlp.up_proj',\n",
       " 'model.layers.8.mlp.act_fn',\n",
       " 'model.layers.9.mlp.up_proj',\n",
       " 'model.layers.9.mlp.act_fn',\n",
       " 'model.layers.10.mlp.up_proj',\n",
       " 'model.layers.10.mlp.act_fn',\n",
       " 'model.layers.11.mlp.up_proj',\n",
       " 'model.layers.11.mlp.act_fn',\n",
       " 'model.layers.12.mlp.up_proj',\n",
       " 'model.layers.12.mlp.act_fn',\n",
       " 'model.layers.13.mlp.up_proj',\n",
       " 'model.layers.13.mlp.act_fn',\n",
       " 'model.layers.14.mlp.up_proj',\n",
       " 'model.layers.14.mlp.act_fn',\n",
       " 'model.layers.15.mlp.up_proj',\n",
       " 'model.layers.15.mlp.act_fn',\n",
       " 'model.layers.16.mlp.up_proj',\n",
       " 'model.layers.16.mlp.act_fn',\n",
       " 'model.layers.17.mlp.up_proj',\n",
       " 'model.layers.17.mlp.act_fn',\n",
       " 'model.layers.18.mlp.up_proj',\n",
       " 'model.layers.18.mlp.act_fn',\n",
       " 'model.layers.19.mlp.up_proj',\n",
       " 'model.layers.19.mlp.act_fn',\n",
       " 'model.layers.20.mlp.up_proj',\n",
       " 'model.layers.20.mlp.act_fn',\n",
       " 'model.layers.21.mlp.up_proj',\n",
       " 'model.layers.21.mlp.act_fn',\n",
       " 'model.layers.22.mlp.up_proj',\n",
       " 'model.layers.22.mlp.act_fn',\n",
       " 'model.layers.23.mlp.up_proj',\n",
       " 'model.layers.23.mlp.act_fn',\n",
       " 'model.layers.24.mlp.up_proj',\n",
       " 'model.layers.24.mlp.act_fn',\n",
       " 'model.layers.25.mlp.up_proj',\n",
       " 'model.layers.25.mlp.act_fn',\n",
       " 'model.layers.26.mlp.up_proj',\n",
       " 'model.layers.26.mlp.act_fn',\n",
       " 'model.layers.27.mlp.up_proj',\n",
       " 'model.layers.27.mlp.act_fn',\n",
       " 'model.layers.28.mlp.up_proj',\n",
       " 'model.layers.28.mlp.act_fn',\n",
       " 'model.layers.29.mlp.up_proj',\n",
       " 'model.layers.29.mlp.act_fn',\n",
       " 'model.layers.30.mlp.up_proj',\n",
       " 'model.layers.30.mlp.act_fn',\n",
       " 'model.layers.31.mlp.up_proj',\n",
       " 'model.layers.31.mlp.act_fn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include_layers = []\n",
    "for name, _ in model.named_modules():\n",
    "    if \"mlp.act\" in name or \"mlp.up\" in name:\n",
    "        include_layers.append(name)\n",
    "include_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dcc0792-c9aa-46ff-bb4c-60f3e1394663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 15313\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "wiki_dataset = load_dataset(\"Self-GRIT/wikitext-2-raw-v1-preprocessed\", split='train')\n",
    "print(wiki_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a67364c9-f273-489e-8a91-91d7f42a16d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ica_dataset = []\n",
    "sample_num = 40\n",
    "ica_num = 1\n",
    "for i in range(sample_num * ica_num):\n",
    "    ica_dataset.append(wiki_dataset['text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81e44c39-f00b-4b97-9d37-7ce21e15f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmfact.decomposition.canica import CanICA\n",
    "import torch \n",
    "import queue \n",
    "from concurrent.futures  import ThreadPoolExecutor \n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "def z_score_signals(signals):\n",
    "\tif not isinstance(signals, torch.Tensor):\n",
    "\t\traise TypeError(\"Input signals must be a PyTorch tensor.\")\n",
    "\n",
    "\tmean = torch.mean(signals, dim=0)\n",
    "\tstd = torch.std(signals, dim=0)\n",
    "\n",
    "\tsignals = signals - mean\n",
    "\n",
    "\teps = torch.finfo(signals.dtype).eps  # 获取当前数据类型的最小正数\n",
    "\tstd = torch.where(std < eps, torch.tensor(1.0, dtype=signals.dtype, device=signals.device), std)\n",
    "\n",
    "\t# 归一化\n",
    "\tsignals /= std\n",
    "\n",
    "\treturn signals\n",
    "\n",
    "\n",
    "import torch\n",
    "import queue\n",
    "from concurrent.futures  import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    " \n",
    "class SingleLayerAnalysis(LayerOutputExtractor): \n",
    "    def __init__(self, model, include_layers=[\"h.0.attn.c_attn\"],  test=False, device='cpu'): \n",
    "        super().__init__(model, include_layers=include_layers, test=test, device=device)\n",
    "        self.include_layers  = include_layers \n",
    "        self.mixing_  = None \n",
    "        self.origin_mixing_  = None\n",
    "        self.normal_mixing_  = None \n",
    " \n",
    "    def fit(self, inputs, n_components=10, alpha=1.96, random_state=666,\n",
    "            preprocessing=False, total_layer_num=None, method=\"fastica\", \n",
    "            max_iter=200, n_iter=5, norm=True): \n",
    "        \n",
    "        # 确定总层数 \n",
    "        if total_layer_num:\n",
    "            total_layer_num = total_layer_num\n",
    "        else:\n",
    "            total_layer_num = len(self.include_layers)  \n",
    " \n",
    "        # 提取层输出\n",
    "        if type(inputs) == list:\n",
    "            layer_outputs = torch.cat([self.extract_layer_outputs(inp)  for inp in inputs], dim=0) \n",
    "        else: \n",
    "            layer_outputs = self.extract_layer_outputs(inputs) \n",
    " \n",
    "        if preprocessing:\n",
    "            layer_outputs = z_score_signals(layer_outputs).to(torch.float64) \n",
    " \n",
    "        token_num = layer_outputs.shape[0]  \n",
    "        layer_outputs = layer_outputs.reshape(token_num,  total_layer_num, -1) \n",
    " \n",
    "        # 获取GPU设备\n",
    "        num_gpus = torch.cuda.device_count()  \n",
    "        if num_gpus == 0:\n",
    "            raise RuntimeError(\"No GPU available\")\n",
    "        max_workers = min(num_gpus, 10)\n",
    "        device_ids = list(range(max_workers))\n",
    "        device_queue = queue.Queue()\n",
    "        for device_id in device_ids:\n",
    "            device_queue.put(device_id)  \n",
    " \n",
    "        # 定义处理函数 \n",
    "        def process_layer(i):\n",
    "            device = device_queue.get() \n",
    "            try: \n",
    "                with torch.cuda.device(device):  \n",
    "                    data = layer_outputs[:, i, :].to(device) \n",
    "                    if method == \"fastica\": \n",
    "                        # CPU处理逻辑（略） \n",
    "                        pass \n",
    "                    else: \n",
    "                        ica = CanICA(n_components=n_components,\n",
    "                                     random_state=random_state, \n",
    "                                     device=device) \n",
    "                        ica.fit(data,  max_iter=max_iter)\n",
    "                        return ica.normal_mixing_ \n",
    "            finally:\n",
    "                device_queue.put(device)  \n",
    " \n",
    "        # 并行执行\n",
    "        normal_mixing_list = []\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = [executor.submit(process_layer, i) for i in range(total_layer_num)] \n",
    "            for future in tqdm(futures, total=total_layer_num, desc=\"Processing Layers\"): \n",
    "                normal_mixing_list.append(future.result().detach().cpu()) \n",
    " \n",
    "        self.normal_mixing_  = torch.cat(normal_mixing_list,  dim=1)\n",
    "\n",
    "        \n",
    "# class SingleLayerAnalysis(LayerOutputExtractor):\n",
    "#     def __init__(self, model, include_layers=[\"h.0.attn.c_attn\"], test=False, device='cpu'):\n",
    "#         super().__init__(model, include_layers=include_layers, test=test, device=device)\n",
    "#         self.include_layers = include_layers\n",
    "#         self.mixing_ = None\n",
    "#         self.origin_mixing_ = None\n",
    "#         self.normal_mixing_ = None\n",
    "\n",
    "#     def fit(self, inputs, n_components=10, alpha=1.96, random_state=666,\n",
    "#             preprocessing=False, total_layer_num=None, method=\"fastica\",\n",
    "#             max_iter=200, n_iter=5, norm=True):\n",
    "#         if total_layer_num:\n",
    "#             total_layer_num = total_layer_num\n",
    "#         else:\n",
    "#             total_layer_num = len(self.include_layers)\n",
    "\n",
    "#         if type(inputs) == list:\n",
    "#             total_layer_outputs = []\n",
    "#             for inp in inputs:\n",
    "#                 layer_outputs = self.extract_layer_outputs(inp)\n",
    "#                 total_layer_outputs.append(layer_outputs)\n",
    "#             layer_outputs = torch.cat(total_layer_outputs, dim=0)\n",
    "#         else:\n",
    "#             layer_outputs = self.extract_layer_outputs(inputs)\n",
    "\n",
    "#         if preprocessing:\n",
    "#             layer_outputs = z_score_signals(layer_outputs)\n",
    "#             layer_outputs = layer_outputs.to(torch.float64)\n",
    "\n",
    "#         token_num = layer_outputs.shape[0]\n",
    "\n",
    "#         layer_outputs = layer_outputs.reshape(token_num, total_layer_num, -1)\n",
    "\n",
    "#         # mixing_list = []\n",
    "#         # origin_mixing_list = []\n",
    "#         normal_mixing_list = []\n",
    "#         for i in trange(total_layer_num):\n",
    "#             if method == \"fastica\":\n",
    "#                 ica = FastICA(n_components=n_components,\n",
    "#                               random_state=random_state,\n",
    "#                               max_iter=max_iter)\n",
    "#                 ica.fit(layer_outputs[:, i, :])\n",
    "#                 mixing = torch.tensor(ica.mixing_.T)\n",
    "#                 # origin_mixing_list.append(mixing)\n",
    "\n",
    "#                 mean = torch.mean(mixing, dim=1, keepdim=True)\n",
    "#                 std = torch.std(mixing, dim=1, keepdim=True)\n",
    "\n",
    "#                 normalized_matrix = (mixing - mean) / std\n",
    "\n",
    "#                 normal_mixing_list.append(normalized_matrix)\n",
    "\n",
    "#             else:\n",
    "#                 ica = CanICA(n_components=n_components, random_state=random_state, device=self.device)\n",
    "#                 ica.fit(layer_outputs[:, i, :], max_iter=max_iter)\n",
    "\n",
    "#                 normal_mixing_list.append(ica.normal_mixing_)\n",
    "                \n",
    "#         self.normal_mixing_ = torch.cat(normal_mixing_list, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d28914bf-39b5-483a-b675-d5cf0dd882e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MutiICA(model, include_layers, dataset, n_components=256, window_size=3, preprocessing=True, max_iter=500, n_iter=5, norm=True):\n",
    "    extractor = MutiLayerAnalysis2(model, include_layers=include_layers, device=model.device)\n",
    "    inputs_list = [tokenizer(inputs, return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset]\n",
    "    extractor.fit(inputs=inputs_list, n_components=n_components, \n",
    "                  window_size=window_size, random_state=666,\n",
    "                  preprocessing=preprocessing, total_layer_num=32, method=\"canica\", max_iter=max_iter, norm=norm, n_iter=n_iter)\n",
    "    return extractor\n",
    "    \n",
    "\n",
    "def save_fbn(data, save_dir, data_type, model_name, n_components, alpha):\n",
    "    if isinstance(data, list) or isinstance(data, dict):\n",
    "        save_path = save_dir + data_type + \"_\" + model_name + \"_\" + str(n_components) + \"_\" + str(alpha) + \".pth\"\n",
    "        print(f\"save at {save_path}\")\n",
    "        torch.save(data, save_path)\n",
    "    else:\n",
    "        save_path = save_dir + data_type + \"_\" + model_name + \"_\" + str(n_components) + \"_\" + str(alpha) + \".pth\"\n",
    "        print(f\"save at {save_path}\")\n",
    "        torch.save(data, save_path)\n",
    "\n",
    "def SingleICA(model, include_layers, dataset, n_components=256, preprocessing=True, max_iter=500, norm=True, n_iter=5):\n",
    "    extractor = SingleLayerAnalysis(model, include_layers=include_layers, device=model.device)\n",
    "    inputs_list = [tokenizer(inputs, return_tensors=\"pt\", max_length=1024, truncation=True) for inputs in dataset]\n",
    "    extractor.fit(inputs=inputs_list, n_components=n_components, \n",
    "                  random_state=327, preprocessing=True,\n",
    "                  total_layer_num=32, method=\"canica\", max_iter=max_iter, norm=norm, n_iter=n_iter)\n",
    "    return extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38b1bf32-3af2-43f6-8076-991740721db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ica_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "752b938a-b095-480f-990a-d86a03f72cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Processing Layers:   0%|          | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "Processing Layers:   3%|▎         | 1/32 [01:08<35:23, 68.51s/it]\u001b[A\n",
      "Processing Layers:   9%|▉         | 3/32 [01:08<08:38, 17.90s/it]\u001b[A\n",
      "Processing Layers:  12%|█▎        | 4/32 [01:09<05:38, 12.08s/it]\u001b[A\n",
      "Processing Layers:  16%|█▌        | 5/32 [01:10<03:39,  8.13s/it]\u001b[A\n",
      "Processing Layers:  19%|█▉        | 6/32 [01:11<02:31,  5.84s/it]\u001b[A\n",
      "Processing Layers:  22%|██▏       | 7/32 [02:01<08:14, 19.77s/it]\u001b[A\n",
      "Processing Layers:  25%|██▌       | 8/32 [02:05<05:55, 14.81s/it]\u001b[A\n",
      "Processing Layers:  28%|██▊       | 9/32 [02:06<04:05, 10.67s/it]\u001b[A\n",
      "Processing Layers:  31%|███▏      | 10/32 [02:06<02:45,  7.53s/it]\u001b[A\n",
      "Processing Layers:  34%|███▍      | 11/32 [02:08<01:59,  5.71s/it]\u001b[A\n",
      "Processing Layers:  38%|███▊      | 12/32 [02:09<01:25,  4.29s/it]\u001b[A\n",
      "Processing Layers:  41%|████      | 13/32 [02:58<05:41, 17.95s/it]\u001b[A\n",
      "Processing Layers:  44%|████▍     | 14/32 [03:03<04:11, 13.98s/it]\u001b[A\n",
      "Processing Layers:  50%|█████     | 16/32 [03:04<02:05,  7.83s/it]\u001b[A\n",
      "Processing Layers:  56%|█████▋    | 18/32 [03:06<01:09,  4.96s/it]\u001b[A\n",
      "Processing Layers:  59%|█████▉    | 19/32 [03:56<03:16, 15.09s/it]\u001b[A\n",
      "Processing Layers:  62%|██████▎   | 20/32 [04:01<02:29, 12.48s/it]\u001b[A\n",
      "Processing Layers:  66%|██████▌   | 21/32 [04:01<01:44,  9.46s/it]\u001b[A\n",
      "Processing Layers:  69%|██████▉   | 22/32 [04:03<01:14,  7.43s/it]\u001b[A\n",
      "Processing Layers:  72%|███████▏  | 23/32 [04:04<00:49,  5.48s/it]\u001b[A\n",
      "Processing Layers:  75%|███████▌  | 24/32 [04:04<00:33,  4.14s/it]\u001b[A\n",
      "Processing Layers:  78%|███████▊  | 25/32 [04:56<02:04, 17.74s/it]\u001b[A\n",
      "Processing Layers:  81%|████████▏ | 26/32 [04:59<01:20, 13.46s/it]\u001b[A\n",
      "Processing Layers:  84%|████████▍ | 27/32 [05:01<00:50, 10.05s/it]\u001b[A\n",
      "Processing Layers:  88%|████████▊ | 28/32 [05:01<00:28,  7.19s/it]\u001b[A\n",
      "Processing Layers:  91%|█████████ | 29/32 [05:02<00:16,  5.36s/it]\u001b[A\n",
      "Processing Layers:  94%|█████████▍| 30/32 [05:02<00:07,  3.91s/it]\u001b[A\n",
      "Processing Layers:  97%|█████████▋| 31/32 [05:53<00:17, 17.76s/it]\u001b[A\n",
      "Processing Layers: 100%|██████████| 32/32 [05:57<00:00, 11.16s/it]\u001b[A\n",
      "100%|██████████| 1/1 [07:00<00:00, 420.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save at ./data/FBN/text40-mlp.act-CanICA-SingleICA-max_iter-300_vicuna-7b-v1.5-muti-layer-wise_128_normal_mixing_std_True.pth\n",
      "CPU times: user 30min 32s, sys: 11min 15s, total: 41min 48s\n",
      "Wall time: 7min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 704512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm import trange\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "model_name = \"vicuna-7b-v1.5-muti-layer-wise\"\n",
    "n_components = 128\n",
    "preprocessing = True\n",
    "max_iter = 300\n",
    "norm = False\n",
    "n_iter = 4\n",
    "\n",
    "normal_list = []\n",
    "for i in trange(ica_num):\n",
    "    extractor = SingleICA(model, include_layers, ica_dataset[sample_num*i:sample_num*(i+1)],\n",
    "                          n_components=n_components, preprocessing=True,\n",
    "                          max_iter=max_iter, norm=norm, n_iter=n_iter)\n",
    "    normal = extractor.normal_mixing_\n",
    "    normal_list.append(normal)\n",
    "normal_components = torch.cat(normal_list, dim=0)\n",
    "save_fbn(normal_components, \"./data/FBN/\", f\"text{len(ica_dataset)}-mlp.act-CanICA-SingleICA-max_iter-{max_iter}\", model_name, n_components, f\"normal_mixing_std_{preprocessing}\")\n",
    "normal_components.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0777d38e-7c70-41fd-b25e-d884a99521f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 704512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_components.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "415bb764-47f2-4b2e-b7c6-140a0e89791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6806b6f6-d0da-4cc5-a4c3-2607d850197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_par_num(neuron_num_list):\n",
    "    total_par = 6738415616\n",
    "    print(\"total parameters:\", total_par)\n",
    "    \n",
    "    total_mlp = 32 * (4096 * 11008 * 3)\n",
    "    print(\"total mlp parameters:\", total_mlp)\n",
    "\n",
    "    total_cut = 0\n",
    "    for i in neuron_num_list:\n",
    "        cut_num = 4096 * 11008 * 3 - (i * 4096 * 3)\n",
    "        total_cut += cut_num\n",
    "    print(\"total cut parameters num:\", total_cut)\n",
    "\n",
    "    print(f\"total cut mlp parameters: {total_cut / total_mlp:.4f}\")\n",
    "    print(f\"total cut parameters: {total_cut / total_par:.4f}\")\n",
    "    print(f\"parameters after cut: {total_par - total_cut:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ffb1f22-8c67-4f38-b728-d14174282ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(297147)\n",
      "tensor(246283.)\n",
      "tensor([11008., 11008., 11008.,  5921.,  6619.,  6827.,  7164.,  7679.,  7961.,\n",
      "         8265.,  8232.,  8168.,  8122.,  7995.,  8011.,  7877.,  7736.,  7457.,\n",
      "         7231.,  6893.,  6777.,  6550.,  6562.,  6246.,  6292.,  5924.,  6004.,\n",
      "         6228.,  6153.,  6349., 11008., 11008.])\n",
      "total parameters: 6738415616\n",
      "total mlp parameters: 4328521728\n",
      "total cut parameters num: tensor(1.3022e+09)\n",
      "total cut mlp parameters: 0.3008\n",
      "total cut parameters: 0.1932\n",
      "parameters after cut: 5436219392.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 11008])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any_mask = torch.abs(normal_components) > 3.46\n",
    "any_mask = torch.any(any_mask, dim=0).reshape(1, -1)\n",
    "print(any_mask.sum())\n",
    "\n",
    "mask = any_mask.reshape(32, 2, -1)\n",
    "mask_matrix = torch.ones((32, 11008))\n",
    "for i in range(3, mask.shape[0] - 2):\n",
    "    mask_matrix[i] = torch.any(mask[i], dim=0)\n",
    "print(mask_matrix.sum())\n",
    "print(mask_matrix.sum(dim=1))\n",
    "cut_par_num(mask_matrix.sum(dim=1))\n",
    "# mask_matrix = np.repeat(mask_matrix, 2, axis=0)\n",
    "mask_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6df00141-85f6-4c4d-9d2e-73fe94d054a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def pruned_llama_mlp(model, mask):\n",
    "    mask = torch.tensor(mask, dtype=torch.bool)\n",
    "\n",
    "    for i in range(len(model.model.layers)):\n",
    "        layer = model.model.layers[i]\n",
    "\n",
    "        mask_1 = mask[i].type(torch.bool)\n",
    "        # pruned_mlp = PrunedLlamaMLP(config=model.config,\n",
    "        #                             mask=mask_1,\n",
    "        #                             device=next(layer.parameters()).device)\n",
    "        with torch.no_grad():\n",
    "            # w1 = layer.mlp.gate_proj.weight[mask_1]\n",
    "            layer.mlp.up_proj.weight.data = layer.mlp.up_proj.weight.data[torch.where(mask_1)[0]]\n",
    "            # pruned_mlp.gate_proj.weight.copy_(w1.contiguous())\n",
    "\n",
    "            # w2 = layer.mlp.up_proj.weight[mask_1]\n",
    "            layer.mlp.gate_proj.weight.data = layer.mlp.gate_proj.weight.data[torch.where(mask_1)[0]]\n",
    "            # pruned_mlp.up_proj.weight.copy_(w2.contiguous())\n",
    "\n",
    "            layer.mlp.up_proj.out_features = mask_1.sum().item()\n",
    "            layer.mlp.gate_proj.out_features = mask_1.sum().item()\n",
    "            layer.mlp.intermediate_size = mask_1.sum().item()\n",
    "\n",
    "            output_weight = layer.mlp.down_proj.weight.data[:, torch.where(mask_1)[0]]\n",
    "\n",
    "            layer.mlp.down_proj.weight.data = output_weight\n",
    "\n",
    "            layer.mlp.down_proj.in_features = mask_1.sum().item()\n",
    "\n",
    "            # w3 = layer.mlp.down_proj.weight[:, mask_1]\n",
    "            # pruned_mlp.down_proj.weight.copy_(w3.contiguous())\n",
    "\n",
    "            # # del layer.mlp\n",
    "            # del w1\n",
    "            # del w2\n",
    "            # del w3\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # layer.mlp = pruned_mlp\n",
    "\n",
    "    return model\n",
    "\n",
    "class PrunedLlamaModel:\n",
    "    def __init__(self, model, mask=None):\n",
    "        self.mask = mask\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self):\n",
    "        total_par = 0\n",
    "        for par in self.model.parameters():\n",
    "            total_par += par.numel()\n",
    "        print(f\"total parameters before pruned: {total_par}\")\n",
    "        self.model = pruned_llama_mlp(self.model, self.mask)\n",
    "\n",
    "        total_par_pruned = 0\n",
    "        for par in self.model.parameters():\n",
    "            total_par_pruned += par.numel()\n",
    "        print(f\"total parameters after pruned: {total_par_pruned}\")\n",
    "        print(f\"total cut num: {total_par - total_par_pruned}\")\n",
    "        print(f\"pruned rate: {(total_par - total_par_pruned) / total_par:.4f}\")\n",
    "\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c542371-26d9-4262-ac3c-a247e2f16a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters before pruned: 6738415616\n",
      "total parameters after pruned: 5436219392\n",
      "total cut num: 1302196224\n",
      "pruned rate: 0.1932\n"
     ]
    }
   ],
   "source": [
    "# from llmfact.pruner.pruner import PrunedLlamaModel\n",
    "pruner = PrunedLlamaModel(model, mask_matrix)\n",
    "model = pruner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "601e9fd5-9d1e-428a-83d1-a37b7dd74e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n"
     ]
    }
   ],
   "source": [
    "from lm_eval import evaluator\n",
    "import lm_eval\n",
    "wrapper_model = lm_eval.models.huggingface.HFLM(pretrained=model, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dd73400-539f-48a1-8d66-d3e605935b80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Task: wikitext] metric word_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\n",
      "[Task: wikitext] metric word_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\n",
      "[Task: wikitext] metric byte_perplexity is defined, but aggregation is not. using default aggregation=weighted_perplexity\n",
      "[Task: wikitext] metric byte_perplexity is defined, but higher_is_better is not. using default higher_is_better=False\n",
      "[Task: wikitext] metric bits_per_byte is defined, but aggregation is not. using default aggregation=bits_per_byte\n",
      "[Task: wikitext] metric bits_per_byte is defined, but higher_is_better is not. using default higher_is_better=False\n",
      "Overwriting default num_fewshot of wikitext from None to 0\n",
      "100%|██████████| 62/62 [00:00<00:00, 433.48it/s]\n",
      "  0%|          | 0/62 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (5943 > 4096). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 62/62 [00:00<00:00, 69.01it/s]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:02<00:00,  2.21s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.10s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.07s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:04<00:00,  4.61s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.04s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.09s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.07s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:03<00:00,  3.84s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:04<00:00,  4.65s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.20s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.08s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.05s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.05s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:04<00:00,  4.49s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.08s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.08s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.09s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.24s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:02<00:00,  2.91s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:05<00:00,  5.60s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.07s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.07s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.10s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:04<00:00,  4.07s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:03<00:00,  3.09s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.15s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.15s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.10s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.12s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.09s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.09s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.09s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.09s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.16s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:02<00:00,  2.27s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.12s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.09s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.10s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.10s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.09s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.10s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:04<00:00,  4.58s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.13s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.12s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.09s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.11s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.12s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.10s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.10s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.99s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  8.67it/s]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  2.07it/s]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.12s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:02<00:00,  2.75s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.11s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.11s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:03<00:00,  3.08s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.11s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.09s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.11s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.17s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.11s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:02<00:00,  2.85s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.10s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.11s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.11s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.16s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.09s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.11s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.09s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.11s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.13s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.10s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.12s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.10s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:04<00:00,  4.50s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.92s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.13s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.13s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:04<00:00,  4.05s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.99s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.13s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.20s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.12s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:02<00:00,  2.45s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:02<00:00,  2.69s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:05<00:00,  5.60s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:05<00:00,  5.31s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.92s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:01<00:00,  1.62s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.11s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.20s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.18s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.11s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.12s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.08s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.11s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.11s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.10s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n",
      "Running loglikelihood requests: 100%|██████████| 1/1 [00:06<00:00,  6.15s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'wikitext': {'alias': 'wikitext',\n",
       "  'word_perplexity,none': 18.94210997911784,\n",
       "  'word_perplexity_stderr,none': 'N/A',\n",
       "  'byte_perplexity,none': 1.7333468983928229,\n",
       "  'byte_perplexity_stderr,none': 'N/A',\n",
       "  'bits_per_byte,none': 0.7935604130138104,\n",
       "  'bits_per_byte_stderr,none': 'N/A'}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = evaluator.simple_evaluate( \n",
    "    model=wrapper_model,\n",
    "    model_args=\"lmsys/vicuna-7b-v1.5\",\n",
    "    tasks=[\"wikitext\"],\n",
    "    num_fewshot=0,\n",
    "    task_manager=lm_eval.tasks.TaskManager(),\n",
    "    batch_size=1)\n",
    "results['results']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
